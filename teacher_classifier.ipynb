{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c4ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, magic, shutil\n",
    "from glob import glob\n",
    "import time, datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import datetime as dt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, gc\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "#from skimage import io\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, confusion_matrix, classification_report\n",
    "from sklearn import metrics, preprocessing\n",
    "from scipy.ndimage import zoom\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d0796-1f1d-40df-ad4a-21f57ed7fe35",
   "metadata": {},
   "source": [
    "#### Hyper Param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c0283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 42,\n",
    "    'model': 'resnet152',\n",
    "    'img_size': 260,\n",
    "    'epochs': 200,\n",
    "    'train_bs':64,\n",
    "    'valid_bs':64,\n",
    "    'lr': 1e-4, ## learning rate\n",
    "    'num_workers': 8,\n",
    "    'verbose_step': 1,\n",
    "    'patience' : 5,\n",
    "    'label_encoder':False,\n",
    "    'device': 'cuda:0',\n",
    "    'freezing': False,\n",
    "    'trainable_layer': 12,\n",
    "    'model_path': './models'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066a8a1-7060-4b6d-bf0c-d4164820a414",
   "metadata": {},
   "source": [
    "#### wandb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127461e4-c16d-47fd-b983-556c12ad0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'temp'\n",
    "time_now = dt.datetime.now()\n",
    "run_id = time_now.strftime(\"%Y%m%d%H%M\")\n",
    "project_name = 'KD_'+ CFG['model'] + '_' + category\n",
    "user = 'hojunking'\n",
    "run_name = project_name + '_' + run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6c4553-4301-4ba8-801e-e7130ac0b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 10Kwalk 1467\n",
      "label: battery 2103\n",
      "label: box 5907\n",
      "label: else 2838\n",
      "label: toothcup 2105\n",
      "label: tumbler 2508\n",
      "label: handkerchief 1824\n",
      "label: milk 1781\n",
      "label: paper 1249\n",
      "label: pet 5199\n",
      "label: plug 2198\n",
      "label: receipt 632\n",
      "label: shopping bag 1284\n",
      "label: stairs 3045\n",
      "label: transportation 1677\n",
      "label: trash picking 1221\n",
      "label: green dish 1261\n",
      "label: leftover 1483\n",
      "label: wrap 1266\n",
      "Train_Images:  41048\n",
      "Train_Images_labels: 41048\n"
     ]
    }
   ],
   "source": [
    "# TRAIN DATASET DATAFRAME\n",
    "train_path = '../Data/carbon_reduction_data/train/'\n",
    "label_list = [\"10Kwalk\",\"battery\",'box','else','toothcup', 'tumbler','handkerchief',\n",
    "              'milk', 'paper', 'pet','plug','receipt', 'shopping bag', 'stairs',\n",
    "             'transportation', 'trash picking', 'green dish','leftover','wrap']\n",
    "\n",
    "train_img_paths = []\n",
    "train_img_labels = []\n",
    "\n",
    "for label in label_list: ## 각 레이블 돌기\n",
    "    print(f'label: {label}',end=' ')\n",
    "    img_paths = [] \n",
    "    img_labels = []\n",
    "\n",
    "    dir_path = train_path + label ## 레이블 폴더 경로\n",
    "    \n",
    "    for folder, subfolders, filenames in os.walk(dir_path): ## 폴더 내 모든 파일 탐색\n",
    "        for img in filenames: ## 각 파일 경로, 레이블 저장\n",
    "            img_paths.append(folder+'/'+img)\n",
    "            img_labels.append(label)\n",
    "        \n",
    "    print(len(img_paths))\n",
    "\n",
    "    train_img_paths.extend(img_paths)\n",
    "    train_img_labels.extend(img_labels)\n",
    "\n",
    "print('Train_Images: ',len(train_img_paths))\n",
    "print(\"Train_Images_labels:\", len(train_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aef7874-aa53-4c21-b0ff-f8985c3dc8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 10Kwalk 489\n",
      "label: battery 702\n",
      "label: box 1969\n",
      "label: else 946\n",
      "label: toothcup 702\n",
      "label: tumbler 837\n",
      "label: handkerchief 609\n",
      "label: milk 594\n",
      "label: paper 417\n",
      "label: pet 1734\n",
      "label: plug 733\n",
      "label: receipt 211\n",
      "label: shopping bag 429\n",
      "label: stairs 1016\n",
      "label: transportation 559\n",
      "label: trash picking 407\n",
      "label: green dish 421\n",
      "label: leftover 495\n",
      "label: wrap 423\n",
      "Test_Images:  13693\n",
      "Test_Images_labels: 13693\n"
     ]
    }
   ],
   "source": [
    "# TEST DATASET DATAFRAME\n",
    "test_path = '../Data/carbon_reduction_data/test/'\n",
    "test_img_paths = []\n",
    "test_img_labels = []\n",
    "\n",
    "for label in label_list: ## 각 레이블 돌기\n",
    "    print(f'label: {label}',end=' ')\n",
    "    img_paths = [] \n",
    "    img_labels = []\n",
    "    dir_path = test_path + label ## 레이블 폴더 경로\n",
    "    \n",
    "    for folder, subfolders, filenames in os.walk(dir_path): ## 폴더 내 모든 파일 탐색\n",
    "        for img in filenames: ## 각 파일 경로, 레이블 저장\n",
    "            img_paths.append(folder+'/'+img)\n",
    "            img_labels.append(label)\n",
    "        \n",
    "    print(len(img_paths))\n",
    "\n",
    "    test_img_paths.extend(img_paths)\n",
    "    test_img_labels.extend(img_labels)\n",
    "\n",
    "print('Test_Images: ',len(test_img_paths))\n",
    "print(\"Test_Images_labels:\", len(test_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2716dd8b-84db-4707-80e2-19b29eae9c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0720.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1028.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1322.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>2893.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>1512.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>2070.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>2728.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5199 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                                dir      label\n",
       "0     0720.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "1     0282.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "2     1028.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "3     1322.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "4     0540.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "...        ...                                                ...        ...\n",
       "5194  2893.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5195  1059.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5196  1512.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5197  2070.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5198  2728.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "\n",
       "[5199 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas 데이터프레임 만들기\n",
    "trn_df = pd.DataFrame(train_img_paths, columns=['image_id'])\n",
    "trn_df['dir'] = trn_df['image_id'].apply(lambda x: os.path.dirname(x))\n",
    "trn_df['image_id'] = trn_df['image_id'].apply(lambda x: os.path.basename(x))\n",
    "trn_df['label'] = train_img_labels\n",
    "train = trn_df\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "105ae88a-4d4f-49a9-bffc-8689e2db0826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0466.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0190.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0234.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>0011.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>0868.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>0611.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0692.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                               dir      label\n",
       "0     0282.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "1     0540.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "2     0466.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "3     0190.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "4     0234.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "...        ...                                               ...        ...\n",
       "1729  0011.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1730  0868.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1731  0611.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1732  0692.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1733  1059.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "\n",
       "[1734 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas 데이터프레임 만들기\n",
    "tst_df = pd.DataFrame(test_img_paths, columns=['image_id'])\n",
    "tst_df['dir'] = tst_df['image_id'].apply(lambda x: os.path.dirname(x))\n",
    "tst_df['image_id'] = tst_df['image_id'].apply(lambda x: os.path.basename(x))\n",
    "tst_df['label'] = test_img_labels\n",
    "test = tst_df\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf387b57-bb5d-4564-96db-d764e547a93a",
   "metadata": {},
   "source": [
    "##### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03c67d39-7933-4f98-b998-54a6036ff4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0720.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1028.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1322.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>2893.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>1512.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>2070.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>2728.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5199 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                                dir  label\n",
       "0     0720.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "1     0282.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "2     1028.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "3     1322.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "4     0540.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "...        ...                                                ...    ...\n",
       "5194  2893.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5195  1059.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5196  1512.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5197  2070.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5198  2728.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "\n",
       "[5199 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'].values)\n",
    "test['label'] = le.transform(test['label'].values)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29be149b-1ea0-4e34-846e-7395d4e94f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding_classes():\n",
    "    # define certain classes to transform differently\n",
    "    capture_image_classes = ['10Kwalk', 'battery','receipt']\n",
    "    return le.transform(capture_image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c454bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97a3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path, sub_path=None):\n",
    "    try:\n",
    "        im_bgr = cv2.imread(path)\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "        past_path = path\n",
    "    except: ## 이미지 에러 발생 시 백지로 대체\n",
    "        im_bgr = cv2.imread('../Data/carbon_reduction/temp_img.jpg')\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884eb987-4341-4fe7-8094-6e61cb051af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.Compose([\n",
    "            A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.75),ratio=(0.90, 1.10)),\n",
    "        ], p=0.8),\n",
    "        A.Compose([\n",
    "            A.Resize(p=1, height = CFG['img_size'], width = CFG['img_size']),\n",
    "        ], p=0.2),\n",
    "    ], p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.SafeRotate(p=0.5, limit=(-20, 20), interpolation=2, border_mode=0, value=(0, 0, 0), mask_value=None),\n",
    "    A.ColorJitter(always_apply=True, p=0.5, contrast=0.2, saturation=0.3, hue=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0),\n",
    "    A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "transform_train_cap = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.Compose([\n",
    "            A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.85),ratio=(0.90, 1.10)),\n",
    "        ], p=0.6),\n",
    "        A.Compose([\n",
    "            A.Resize(p=1, height = CFG['img_size'], width = CFG['img_size']),\n",
    "        ], p=0.4),\n",
    "    ], p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0),\n",
    "    A.pytorch.transforms.ToTensorV2()\n",
    "])\n",
    "\n",
    "transform_test = A.Compose([\n",
    "    A.Resize(height = CFG['img_size'], width = CFG['img_size']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "    A.pytorch.transforms.ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1561be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, data_root, transform=None, transform2=None, output_label=True, encoded_class=False):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transform = transform\n",
    "        self.transform2 = transform2\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "         \n",
    "        if encoded_class == True:\n",
    "            self.encoded_class = label_encoding_classes()\n",
    "        else:\n",
    "            self.encoded_class = encoded_class\n",
    "            \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "        \n",
    "    # AUGMENTATION DIFFERENTLY DEPENDING ON THE TARGET\n",
    "    def custom_augmentation(self, img, target):\n",
    "        if self.encoded_class is not False and target in self.encoded_class:\n",
    "            return self.transform2(image=img)\n",
    "        else:\n",
    "            return self.transform(image=img)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # GET IMAGES\n",
    "        path = \"{}/{}\".format(self.data_root[index], self.df.iloc[index]['image_id'])\n",
    "        img  = get_img(path)\n",
    "        \n",
    "        # GET LABELS\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "            \n",
    "            # CUSTOM AUGMENTATION\n",
    "            transformed = self.custom_augmentation(img, target) \n",
    "            img = transformed['image']\n",
    "            return img, target\n",
    "        else:\n",
    "            transformed =self.transform(image=img)\n",
    "            img = transformed['image']\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7acf9a37-cf66-431c-85c2-d559515afe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED MODEL\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self, model_arch_str, num_classes= 2,pretrained=True):\n",
    "        super(Teacher, self).__init__()\n",
    "        model_arch = getattr(models, model_arch_str)\n",
    "        self.backbone = model_arch(pretrained=pretrained) ## 모델 선언 여기 models.##(pretrained =pretrained)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "240e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root=train.dir.values):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "    train_data_root = data_root[trn_idx]\n",
    "    valid_data_root = data_root[val_idx]\n",
    "    \n",
    "        \n",
    "    train_ds = CustomDataset(train_, train_data_root, transform=transform_train,\n",
    "                            transform2=transform_train_cap, output_label=True, encoded_class=CFG['label_encoder'])\n",
    "    valid_ds = CustomDataset(valid_, valid_data_root, transform=transform_test,\n",
    "                            output_label=True)\n",
    "    # WEIGHTEDRANDOMSAMPLER\n",
    "    class_counts = train_.label.value_counts(sort=False).to_dict()\n",
    "    num_samples = sum(class_counts.values())\n",
    "    print(f'cls_cnts: {len(class_counts)}\\nnum_samples:{num_samples}')\n",
    "    \n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "    t_labels = train_.label.to_list()\n",
    "    \n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))]\n",
    "\n",
    "\n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "\n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))] \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        sampler=sampler, \n",
    "        num_workers=CFG['num_workers']\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ffa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    # SET MODEL TRAINING MODE\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = None\n",
    "    loss_sum = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TEACHER MODEL PREDICTION\n",
    "        with torch.cuda.amp.autocast():\n",
    "            image_preds = model(imgs)   #output = model(input)\n",
    "\n",
    "            loss = loss_fn(image_preds, image_labels)\n",
    "            loss_sum+=loss.detach()\n",
    "            \n",
    "            # BACKPROPAGATION\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01    \n",
    "        \n",
    "            # TQDM VERBOSE_STEP TRACKING\n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                pbar.set_description(description)\n",
    "        \n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    \n",
    "    matrix = confusion_matrix(image_targets_all,image_preds_all)\n",
    "    epoch_f1 = f1_score(image_targets_all, image_preds_all, average='macro')\n",
    "    \n",
    "    accuracy = (image_preds_all==image_targets_all).mean()\n",
    "    trn_loss = loss_sum/len(train_loader)\n",
    "    \n",
    "    return image_preds_all, accuracy, trn_loss, matrix, epoch_f1\n",
    "\n",
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
    "    t = time.time()\n",
    "    \n",
    "    # SET MODEL VALID MODE\n",
    "    model.eval()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    avg_loss = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        # TEACHER MODEL PREDICTION\n",
    "        image_preds = model(imgs)\n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(image_preds, image_labels)\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]\n",
    "        \n",
    "        # TQDM\n",
    "        description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    matrix = confusion_matrix(image_targets_all,image_preds_all)\n",
    "    \n",
    "    epoch_f1 = f1_score(image_targets_all, image_preds_all, average='macro')\n",
    "    acc = (image_preds_all==image_targets_all).mean()\n",
    "    val_loss = avg_loss/len(val_loader)\n",
    "    \n",
    "    return image_preds_all, acc, val_loss, matrix, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e9950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        print(f' present score: {score}')\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best F1 score from now: {self.best_score}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37102a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojunking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hojun/git/KD_models/wandb/run-20230520_211812-9df9bu1i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hojunking/KD_resnet152_pet/runs/9df9bu1i' target=\"_blank\">youthful-pine-7</a></strong> to <a href='https://wandb.ai/hojunking/KD_resnet152_pet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hojunking/KD_resnet152_pet' target=\"_blank\">https://wandb.ai/hojunking/KD_resnet152_pet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hojunking/KD_resnet152_pet/runs/9df9bu1i' target=\"_blank\">https://wandb.ai/hojunking/KD_resnet152_pet/runs/9df9bu1i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: resnet152\n",
      "Training start with fold: 0 epoch: 200 \n",
      "\n",
      "cls_cnts: 2\n",
      "num_samples:4159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3807: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:07<00:00,  1.03s/it]\n",
      "epoch 0 loss: 0.0149: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:22<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.07135] Val Loss : [0.01455] Val F1 Score : [0.99682]\n",
      " present score: 0.9968199316896865\n",
      "Epoch 1/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.0238: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:05<00:00,  1.01s/it]\n",
      "epoch 1 loss: 0.0174: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.02798] Val Loss : [0.01669] Val F1 Score : [0.99364]\n",
      " present score: 0.9936357442711499\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9968199316896865\n",
      "Epoch 2/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 loss: 0.0185: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:01<00:00,  1.06it/s]\n",
      "epoch 2 loss: 0.0137: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.02566] Val Loss : [0.01313] Val F1 Score : [0.99576]\n",
      " present score: 0.9957571628474333\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Best F1 score from now: 0.9968199316896865\n",
      "Epoch 3/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 loss: 0.0164: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.04it/s]\n",
      "epoch 3 loss: 0.0183: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.00764] Val Loss : [0.01755] Val F1 Score : [0.99365]\n",
      " present score: 0.9936521112833723\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Best F1 score from now: 0.9968199316896865\n",
      "Epoch 4/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 loss: 0.0120: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.04it/s]\n",
      "epoch 4 loss: 0.0243: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.01341] Val Loss : [0.02327] Val F1 Score : [0.99260]\n",
      " present score: 0.9925988505852297\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Best F1 score from now: 0.9968199316896865\n",
      "Epoch 5/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 loss: 0.0043: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:03<00:00,  1.03it/s]\n",
      "epoch 5 loss: 0.0103: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:20<00:00,  1.22s/it]\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.00520] Val Loss : [0.00996] Val F1 Score : [0.99682]\n",
      " present score: 0.9968199316896865\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Best F1 score from now: 0.9968199316896865\n",
      "stop called\n",
      "time : 0:08:25\n",
      "fold: 0, Best Epoch : 0/ 6\n",
      "Best Train Marco F1 : 0.97860\n",
      "[[2038   43]\n",
      " [  46 2032]]\n",
      "Best Valid Marco F1 : 0.99682\n",
      "[[677   2]\n",
      " [  1 360]]\n",
      "-----------------------------------------------------------------------\n",
      "Training start with fold: 1 epoch: 200 \n",
      "\n",
      "cls_cnts: 2\n",
      "num_samples:4159\n",
      "Fold: 1\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3703: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.10it/s]\n",
      "epoch 0 loss: 0.0105: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:16<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.08417] Val Loss : [0.01003] Val F1 Score : [0.99471]\n",
      " present score: 0.9947134647037355\n",
      "Epoch 1/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 loss: 0.0521: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.03it/s]\n",
      "epoch 1 loss: 0.0124: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.02077] Val Loss : [0.01190] Val F1 Score : [0.99366]\n",
      " present score: 0.9936601859678783\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9947134647037355\n",
      "Epoch 2/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 loss: 0.0196: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.10it/s]\n",
      "epoch 2 loss: 0.0136: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:17<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.02309] Val Loss : [0.01297] Val F1 Score : [0.99577]\n",
      " present score: 0.9957680741889148\n",
      "Epoch 3/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 loss: 0.0123: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:58<00:00,  1.11it/s]\n",
      "epoch 3 loss: 0.0080: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.01981] Val Loss : [0.00763] Val F1 Score : [0.99576]\n",
      " present score: 0.9957626428070862\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9957680741889148\n",
      "Epoch 4/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 loss: 0.0073: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.09it/s]\n",
      "epoch 4 loss: 0.0100: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:16<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.01326] Val Loss : [0.00961] Val F1 Score : [0.99576]\n",
      " present score: 0.9957626428070862\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Best F1 score from now: 0.9957680741889148\n",
      "Epoch 5/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 loss: 0.0119: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:00<00:00,  1.07it/s]\n",
      "epoch 5 loss: 0.0050: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.00966] Val Loss : [0.00480] Val F1 Score : [0.99788]\n",
      " present score: 0.9978813214035431\n",
      "Epoch 6/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6 loss: 0.0048: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.04it/s]\n",
      "epoch 6 loss: 0.0052: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:16<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.00520] Val Loss : [0.00496] Val F1 Score : [0.99788]\n",
      " present score: 0.9978813214035431\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9978813214035431\n",
      "Epoch 7/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7 loss: 0.0047: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.04it/s]\n",
      "epoch 7 loss: 0.0018: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:17<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.00596] Val Loss : [0.00171] Val F1 Score : [0.99894]\n",
      " present score: 0.9989399772298955\n",
      "Epoch 8/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8 loss: 0.0024: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.04it/s]\n",
      "epoch 8 loss: 0.0027: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.00505] Val Loss : [0.00259] Val F1 Score : [0.99788]\n",
      " present score: 0.9978813214035431\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9989399772298955\n",
      "Epoch 9/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 9 loss: 0.0177: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:58<00:00,  1.10it/s]\n",
      "epoch 9 loss: 0.0011: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.00382] Val Loss : [0.00106] Val F1 Score : [1.00000]\n",
      " present score: 1.0\n",
      "Epoch 10/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10 loss: 0.0013: 100%|███████████████████████████████████████████████████████████████████████| 65/65 [01:04<00:00,  1.01it/s]\n",
      "epoch 10 loss: 0.0010: 100%|███████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.00189] Val Loss : [0.00100] Val F1 Score : [1.00000]\n",
      " present score: 1.0\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 1.0\n",
      "Epoch 11/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 11 loss: 0.0008: 100%|███████████████████████████████████████████████████████████████████████| 65/65 [01:00<00:00,  1.08it/s]\n",
      "epoch 11 loss: 0.0041: 100%|███████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [0.00126] Val Loss : [0.00394] Val F1 Score : [0.99788]\n",
      " present score: 0.9978813214035431\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Best F1 score from now: 1.0\n",
      "Epoch 12/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 12 loss: 0.0018: 100%|███████████████████████████████████████████████████████████████████████| 65/65 [01:00<00:00,  1.08it/s]\n",
      "epoch 12 loss: 0.0017: 100%|███████████████████████████████████████████████████████████████████████| 17/17 [00:20<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [0.00115] Val Loss : [0.00159] Val F1 Score : [0.99894]\n",
      " present score: 0.9989399772298955\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Best F1 score from now: 1.0\n",
      "Epoch 13/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 13 loss: 0.0008: 100%|███████████████████████████████████████████████████████████████████████| 65/65 [01:03<00:00,  1.02it/s]\n",
      "epoch 13 loss: 0.0026: 100%|███████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [0.00086] Val Loss : [0.00251] Val F1 Score : [0.99894]\n",
      " present score: 0.9989399772298955\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Best F1 score from now: 1.0\n",
      "Epoch 14/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 14 loss: 0.0017: 100%|███████████████████████████████████████████████████████████████████████| 65/65 [01:01<00:00,  1.06it/s]\n",
      "epoch 14 loss: 0.0031: 100%|███████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.15s/it]\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [0.00409] Val Loss : [0.00297] Val F1 Score : [0.99894]\n",
      " present score: 0.9989399772298955\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Best F1 score from now: 1.0\n",
      "stop called\n",
      "time : 0:20:06\n",
      "fold: 1, Best Epoch : 9/ 15\n",
      "Best Train Marco F1 : 0.99856\n",
      "[[2110    3]\n",
      " [   3 2043]]\n",
      "Best Valid Marco F1 : 1.00000\n",
      "[[679   0]\n",
      " [  0 361]]\n",
      "-----------------------------------------------------------------------\n",
      "Training start with fold: 2 epoch: 200 \n",
      "\n",
      "cls_cnts: 2\n",
      "num_samples:4159\n",
      "Fold: 2\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.3792: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:04<00:00,  1.00it/s]\n",
      "epoch 0 loss: 0.0396: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:21<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.07195] Val Loss : [0.03790] Val F1 Score : [0.99047]\n",
      " present score: 0.9904720702379437\n",
      "Epoch 1/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 loss: 0.0202: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.09it/s]\n",
      "epoch 1 loss: 0.0180: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.02743] Val Loss : [0.01717] Val F1 Score : [0.99682]\n",
      " present score: 0.9968158034126016\n",
      "Epoch 2/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 loss: 0.0204: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:58<00:00,  1.11it/s]\n",
      "epoch 2 loss: 0.0234: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.01371] Val Loss : [0.02235] Val F1 Score : [0.99788]\n",
      " present score: 0.9978813214035431\n",
      "Epoch 3/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 loss: 0.0070: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:00<00:00,  1.08it/s]\n",
      "epoch 3 loss: 0.0250: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.01144] Val Loss : [0.02393] Val F1 Score : [0.99894]\n",
      " present score: 0.9989399772298955\n",
      "Epoch 4/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 loss: 0.0094: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.09it/s]\n",
      "epoch 4 loss: 0.0602: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:20<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.01605] Val Loss : [0.05761] Val F1 Score : [0.99261]\n",
      " present score: 0.9926082290311804\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9989399772298955\n",
      "Epoch 5/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 loss: 0.0045: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:01<00:00,  1.05it/s]\n",
      "epoch 5 loss: 0.0283: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.00694] Val Loss : [0.02709] Val F1 Score : [0.99682]\n",
      " present score: 0.996824023412648\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Best F1 score from now: 0.9989399772298955\n",
      "Epoch 6/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 6 loss: 0.0023: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:57<00:00,  1.12it/s]\n",
      "epoch 6 loss: 0.0234: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.00323] Val Loss : [0.02237] Val F1 Score : [0.99788]\n",
      " present score: 0.9978785814237167\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Best F1 score from now: 0.9989399772298955\n",
      "Epoch 7/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7 loss: 0.0009: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.05it/s]\n",
      "epoch 7 loss: 0.0308: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.00184] Val Loss : [0.02946] Val F1 Score : [0.99894]\n",
      " present score: 0.9989399772298955\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Best F1 score from now: 0.9989399772298955\n",
      "Epoch 8/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 8 loss: 0.0012: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:00<00:00,  1.07it/s]\n",
      "epoch 8 loss: 0.0249: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.15s/it]\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.00055] Val Loss : [0.02380] Val F1 Score : [0.99894]\n",
      " present score: 0.9989399772298955\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Best F1 score from now: 0.9989399772298955\n",
      "stop called\n",
      "time : 0:12:06\n",
      "fold: 2, Best Epoch : 3/ 9\n",
      "Best Train Marco F1 : 0.99615\n",
      "[[2087    8]\n",
      " [   8 2056]]\n",
      "Best Valid Marco F1 : 0.99894\n",
      "[[678   1]\n",
      " [  0 361]]\n",
      "-----------------------------------------------------------------------\n",
      "Training start with fold: 3 epoch: 200 \n",
      "\n",
      "cls_cnts: 2\n",
      "num_samples:4159\n",
      "Fold: 3\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.4197: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:02<00:00,  1.04it/s]\n",
      "epoch 0 loss: 0.0173: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.07659] Val Loss : [0.01695] Val F1 Score : [0.99254]\n",
      " present score: 0.9925407925407925\n",
      "Epoch 1/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 loss: 0.0614: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:01<00:00,  1.05it/s]\n",
      "epoch 1 loss: 0.0066: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.03016] Val Loss : [0.00868] Val F1 Score : [0.99469]\n",
      " present score: 0.994693005687669\n",
      "Epoch 2/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 loss: 0.0115: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.09it/s]\n",
      "epoch 2 loss: 0.0120: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.01660] Val Loss : [0.01152] Val F1 Score : [0.99361]\n",
      " present score: 0.9936106423761858\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.994693005687669\n",
      "Epoch 3/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 loss: 0.0120: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.09it/s]\n",
      "epoch 3 loss: 0.0123: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.01897] Val Loss : [0.01348] Val F1 Score : [0.99681]\n",
      " present score: 0.9968116383375965\n",
      "Epoch 4/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 loss: 0.0150: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:59<00:00,  1.09it/s]\n",
      "epoch 4 loss: 0.0043: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.01587] Val Loss : [0.00448] Val F1 Score : [0.99788]\n",
      " present score: 0.9978758169934641\n",
      "Epoch 5/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 loss: 0.0063: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:07<00:00,  1.04s/it]\n",
      "epoch 5 loss: 0.0046: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:19<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.00989] Val Loss : [0.00454] Val F1 Score : [0.99788]\n",
      " present score: 0.9978758169934641\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9978758169934641\n",
      "Epoch 6/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 6 loss: 0.0045: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [00:58<00:00,  1.11it/s]\n",
      "epoch 6 loss: 0.0023: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.00434] Val Loss : [0.00232] Val F1 Score : [0.99894]\n",
      " present score: 0.9989372127791988\n",
      "Epoch 7/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7 loss: 0.0022: 100%|████████████████████████████████████████████████████████████████████████| 65/65 [01:01<00:00,  1.06it/s]\n",
      "epoch 7 loss: 0.0041: 100%|████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.00255] Val Loss : [0.00407] Val F1 Score : [0.99894]\n",
      " present score: 0.9989372127791988\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Best F1 score from now: 0.9989372127791988\n",
      "Epoch 8/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 8 loss: 0.0022:  32%|███████████████████████▎                                                | 21/65 [00:25<00:38,  1.15it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    # WANDB TRACKER INIT\n",
    "    wandb.init(project=project_name, entity=user)\n",
    "    wandb.config.update(CFG)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.define_metric(\"Train Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train-Valid Accuracy\", step_metric=\"epoch\")\n",
    "    \n",
    "    model_dir = CFG['model_path'] + '/{}'.format(run_name)\n",
    "    train_dir = train.dir.values\n",
    "    best_fold = 0\n",
    "    best_f1 =0.0\n",
    "    print('Model: {}'.format(CFG['model']))\n",
    "    # MAKE MODEL DIR\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # STRATIFIED K-FOLD DEFINITION\n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    # TEST PROCESS FOLD BREAK\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        print(f'Training start with fold: {fold} epoch: {CFG[\"epochs\"]} \\n')\n",
    "\n",
    "        # EARLY STOPPING DEFINITION\n",
    "        early_stopping = EarlyStopping(patience=CFG[\"patience\"], verbose=True)\n",
    "\n",
    "        # DATALOADER DEFINITION\n",
    "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root=train_dir)\n",
    "\n",
    "        # MODEL & DEVICE DEFINITION \n",
    "        device = torch.device(CFG['device'])\n",
    "        model =Teacher(CFG['model'], train.label.nunique(), pretrained=True)\n",
    "        \n",
    "        # MODEL FREEZING\n",
    "        #model.freezing(freeze = CFG['freezing'], trainable_layer = CFG['trainable_layer'])\n",
    "        if CFG['freezing'] ==True:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "        model.to(device)\n",
    "        # MODEL DATA PARALLEL\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler()   \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=5)\n",
    "\n",
    "        # CRITERION (LOSS FUNCTION)\n",
    "        loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        wandb.watch(model, loss_tr, log='all')\n",
    "        train_acc_list = []\n",
    "        train_matrix_list = []\n",
    "        train_f1_list = []\n",
    "        valid_acc_list = []\n",
    "        valid_matrix_list = []\n",
    "        valid_f1_list = []\n",
    "        \n",
    "\n",
    "        start = time.time()\n",
    "        print(f'Fold: {fold}')\n",
    "        for epoch in range(CFG['epochs']):\n",
    "            print('Epoch {}/{}'.format(epoch, CFG['epochs'] - 1))\n",
    "\n",
    "            # TRAINIG\n",
    "            train_preds_all, train_acc, train_loss, train_matrix, train_f1 = train_one_epoch(epoch, model, loss_tr,\n",
    "                                                                        optimizer, train_loader, device, scheduler=scheduler)\n",
    "            wandb.log({'Train Accuracy':train_acc, 'Train Loss' : train_loss, 'Train F1': train_f1, 'epoch' : epoch})\n",
    "\n",
    "            # VALIDATION\n",
    "            with torch.no_grad():\n",
    "                valid_preds_all, valid_acc, valid_loss, valid_matrix, valid_f1= valid_one_epoch(epoch, model, loss_fn,\n",
    "                                                                        val_loader, device, scheduler=None)\n",
    "                wandb.log({'Valid Accuracy':valid_acc, 'Valid Loss' : valid_loss, 'Valid F1': valid_f1 ,'epoch' : epoch})\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{train_loss :.5f}] Val Loss : [{valid_loss :.5f}] Val F1 Score : [{valid_f1:.5f}]')\n",
    "            \n",
    "            # SAVE ALL RESULTS\n",
    "            train_acc_list.append(train_acc)\n",
    "            train_matrix_list.append(train_matrix)\n",
    "            train_f1_list.append(train_f1)\n",
    "\n",
    "            valid_acc_list.append(valid_acc)\n",
    "            valid_matrix_list.append(valid_matrix)\n",
    "            valid_f1_list.append(valid_f1)\n",
    "\n",
    "            # MODEL SAVE (THE BEST MODEL OF ALL OF FOLD PROCESS)\n",
    "            if valid_f1 > best_f1:\n",
    "                best_f1 = valid_f1\n",
    "                best_epoch = epoch\n",
    "                # SAVE WITH DATAPARARELLEL WRAPPER\n",
    "                #torch.save(model.state_dict(), (model_dir+'/{}.pth').format(CFG['model']))\n",
    "                # SAVE WITHOUT DATAPARARELLEL WRAPPER\n",
    "                torch.save(model.module.state_dict(), (model_dir+'/{}.pth').format(CFG['model']))\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            stop = early_stopping(valid_f1)\n",
    "            if stop:\n",
    "                print(\"stop called\")   \n",
    "                break\n",
    "\n",
    "        end = time.time() - start\n",
    "        time_ = str(datetime.timedelta(seconds=end)).split(\".\")[0]\n",
    "        print(\"time :\", time_)\n",
    "\n",
    "        # PRINT BEST F1 SCORE MODEL OF FOLD\n",
    "        best_index = valid_f1_list.index(max(valid_f1_list))\n",
    "        print(f'fold: {fold}, Best Epoch : {best_index}/ {len(valid_f1_list)}')\n",
    "        print(f'Best Train Marco F1 : {train_f1_list[best_index]:.5f}')\n",
    "        print(train_matrix_list[best_index])\n",
    "        print(f'Best Valid Marco F1 : {valid_f1_list[best_index]:.5f}')\n",
    "        print(valid_matrix_list[best_index])\n",
    "        print('-----------------------------------------------------------------------')\n",
    "\n",
    "        # K-FOLD END\n",
    "        if valid_f1_list[best_index] > best_fold:\n",
    "            best_fold = valid_f1_list[best_index]\n",
    "            top_fold = fold\n",
    "    print(f'Best Fold F1 score: {best_fold} Top fold : {top_fold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93dba2e1-92a8-4e81-89e8-17005fb81bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0466.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0190.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0234.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>0074.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>0515.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0011.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>0611.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>0692.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                          dir  label\n",
       "0     0282.jpg  ../Data/carbon_reduction_data/test/toothcup      0\n",
       "1     0540.jpg  ../Data/carbon_reduction_data/test/toothcup      0\n",
       "2     0466.jpg  ../Data/carbon_reduction_data/test/toothcup      0\n",
       "3     0190.jpg  ../Data/carbon_reduction_data/test/toothcup      0\n",
       "4     0234.jpg  ../Data/carbon_reduction_data/test/toothcup      0\n",
       "...        ...                                          ...    ...\n",
       "1534  0074.jpg   ../Data/carbon_reduction_data/test/tumbler      1\n",
       "1535  0515.jpg   ../Data/carbon_reduction_data/test/tumbler      1\n",
       "1536  0011.jpg   ../Data/carbon_reduction_data/test/tumbler      1\n",
       "1537  0611.jpg   ../Data/carbon_reduction_data/test/tumbler      1\n",
       "1538  0692.jpg   ../Data/carbon_reduction_data/test/tumbler      1\n",
       "\n",
       "[1539 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "681ed966-953b-4533-a9b5-1438c1bb27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## inference #############################\n",
    "def inference(model, data_loader, device):\n",
    "    model.eval()\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "\n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f910c08-4857-43b4-b499-b5a7d799e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [00:29<00:00,  1.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0466.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0190.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0234.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>0011.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>0868.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>0611.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0692.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                               dir  label  pred\n",
       "0     0282.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled      1     1\n",
       "1     0540.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled      1     1\n",
       "2     0466.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled      1     1\n",
       "3     0190.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled      1     1\n",
       "4     0234.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled      1     1\n",
       "...        ...                                               ...    ...   ...\n",
       "1729  0011.jpg    ../Data/carbon_reduction_data/bin/test/labeled      0     0\n",
       "1730  0868.jpg    ../Data/carbon_reduction_data/bin/test/labeled      0     0\n",
       "1731  0611.jpg    ../Data/carbon_reduction_data/bin/test/labeled      0     0\n",
       "1732  0692.jpg    ../Data/carbon_reduction_data/bin/test/labeled      0     0\n",
       "1733  1059.jpg    ../Data/carbon_reduction_data/bin/test/labeled      0     0\n",
       "\n",
       "[1734 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN INFERENCE\n",
    "model = Teacher(CFG['model'], test.label.nunique(), pretrained=True)\n",
    "load_model = CFG['model_path'] + '/KD_resnet152_pet_202305202118/' + CFG['model'] + '.pth'\n",
    "test_dir = test.dir.values\n",
    "\n",
    "tst_ds = CustomDataset(test, test_dir, transform=transform_test, output_label=False)\n",
    "tst_loader = torch.utils.data.DataLoader(\n",
    "    tst_ds, \n",
    "    batch_size=CFG['train_bs'],\n",
    "    num_workers=CFG['num_workers'],\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "device = torch.device(CFG['device'])\n",
    "\n",
    "# INFERENCE VIA MULTI-GPU\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#         model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# RUN INFERENCE\n",
    "predictions = []\n",
    "model.load_state_dict(torch.load(load_model))\n",
    "with torch.no_grad():\n",
    "    predictions += [inference(model, tst_loader, device)]\n",
    "\n",
    "\n",
    "predictions = np.mean(predictions, axis=0) \n",
    "test['pred'] = np.argmax(predictions, axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dc9d547-6d2c-4d13-95c6-f9baf6e0f5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0466.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0190.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0234.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>0011.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>0868.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>0611.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0692.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                               dir      label  \\\n",
       "0     0282.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled   \n",
       "1     0540.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled   \n",
       "2     0466.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled   \n",
       "3     0190.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled   \n",
       "4     0234.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled   \n",
       "...        ...                                               ...        ...   \n",
       "1729  0011.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled   \n",
       "1730  0868.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled   \n",
       "1731  0611.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled   \n",
       "1732  0692.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled   \n",
       "1733  1059.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled   \n",
       "\n",
       "           pred  \n",
       "0     unlabeled  \n",
       "1     unlabeled  \n",
       "2     unlabeled  \n",
       "3     unlabeled  \n",
       "4     unlabeled  \n",
       "...         ...  \n",
       "1729    labeled  \n",
       "1730    labeled  \n",
       "1731    labeled  \n",
       "1732    labeled  \n",
       "1733    labeled  \n",
       "\n",
       "[1734 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label'] = le.inverse_transform(test['label'].values)\n",
    "test['pred'] = le.inverse_transform(test['pred'].values)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da647547-7a7b-4690-94df-bb42ca426f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9988\n",
      "f1_score: 0.9987\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_acc = np.sum(test.label == test.pred) / len(test)\n",
    "test_matrix = confusion_matrix(test['label'], test['pred'])\n",
    "epoch_f1 = f1_score(test['label'], test['pred'], average='macro')\n",
    "print(f'accuracy: {test_acc:.4f}')\n",
    "print(f'f1_score: {epoch_f1:.4f}')\n",
    "\n",
    "test_matrix = confusion_matrix(test['label'], test['pred'], normalize='true')\n",
    "#test_matrix = confusion_matrix(test['label'], test['pred'])\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(test_matrix, \n",
    "            annot=True, \n",
    "            xticklabels = sorted(set(test['label'])), \n",
    "            yticklabels = sorted(set(test['label'])),\n",
    "            )\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#print(f'confusion_matrix \\n-------------------------\\n {test_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab89fde5-9498-4a99-8bb8-4af301e4a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_result/incep_res_0403.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8cbfb49-98dd-4d2f-a778-21764270d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d9a4c94-8eb0-49f1-9e94-d412533a498b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bottle            1229\n",
       "dishes            1163\n",
       "stairs             806\n",
       "else               597\n",
       "plug               593\n",
       "battery            501\n",
       "transportation     458\n",
       "handkerchief       443\n",
       "10Kwalk            411\n",
       "pet                410\n",
       "shopping bag       409\n",
       "box                382\n",
       "paper              374\n",
       "milk               371\n",
       "trash picking      325\n",
       "receipt            288\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe7bc385-c882-4830-81f9-e6494fab928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1475945/30508609.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  label.sort_values(by=['image_id'],ascending=True, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10Kwalk_1047.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10Kwalk_1174.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16583193231991658319380893.jpg</td>\n",
       "      <td>../Data/carbon_data/else/instance_spoon</td>\n",
       "      <td>else</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16583311864571658331260534.jpg</td>\n",
       "      <td>../Data/carbon_data/else/instance_spoon</td>\n",
       "      <td>else</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20220722_125118_HDR1658461897787.jpg</td>\n",
       "      <td>../Data/carbon_data/else/instance_spoon</td>\n",
       "      <td>else</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>wrap_990.jpg</td>\n",
       "      <td>../Data/carbon_data/dishes/wrap</td>\n",
       "      <td>dishes</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>wrap_995.jpg</td>\n",
       "      <td>../Data/carbon_data/dishes/wrap</td>\n",
       "      <td>dishes</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>양치컵 사용_063.jpg</td>\n",
       "      <td>../Data/carbon_data/bottle/toothcup/toothcup_852</td>\n",
       "      <td>bottle</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>양치컵 사용_140.jpg</td>\n",
       "      <td>../Data/carbon_data/bottle/toothcup/toothcup_852</td>\n",
       "      <td>bottle</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>양치컵 사용_175.jpg</td>\n",
       "      <td>../Data/carbon_data/bottle/toothcup/toothcup_852</td>\n",
       "      <td>bottle</td>\n",
       "      <td>else</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 image_id  \\\n",
       "0                        10Kwalk_1047.jpg   \n",
       "1                        10Kwalk_1174.jpg   \n",
       "2          16583193231991658319380893.jpg   \n",
       "3          16583311864571658331260534.jpg   \n",
       "4    20220722_125118_HDR1658461897787.jpg   \n",
       "..                                    ...   \n",
       "592                          wrap_990.jpg   \n",
       "593                          wrap_995.jpg   \n",
       "594                        양치컵 사용_063.jpg   \n",
       "595                        양치컵 사용_140.jpg   \n",
       "596                        양치컵 사용_175.jpg   \n",
       "\n",
       "                                                  dir    label  pred  \n",
       "0                         ../Data/carbon_data/10Kwalk  10Kwalk  else  \n",
       "1                         ../Data/carbon_data/10Kwalk  10Kwalk  else  \n",
       "2             ../Data/carbon_data/else/instance_spoon     else  else  \n",
       "3             ../Data/carbon_data/else/instance_spoon     else  else  \n",
       "4             ../Data/carbon_data/else/instance_spoon     else  else  \n",
       "..                                                ...      ...   ...  \n",
       "592                   ../Data/carbon_data/dishes/wrap   dishes  else  \n",
       "593                   ../Data/carbon_data/dishes/wrap   dishes  else  \n",
       "594  ../Data/carbon_data/bottle/toothcup/toothcup_852   bottle  else  \n",
       "595  ../Data/carbon_data/bottle/toothcup/toothcup_852   bottle  else  \n",
       "596  ../Data/carbon_data/bottle/toothcup/toothcup_852   bottle  else  \n",
       "\n",
       "[597 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_list = []\n",
    "label = test[test.pred == 'else']\n",
    "\n",
    "label.sort_values(by=['image_id'],ascending=True, inplace=True)\n",
    "label.reset_index(inplace=True, drop=True)\n",
    "tmp = label['image_id'].value_counts().index.sort_values()\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab3b8b-043d-4d3b-b14f-13564a985766",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show img status\n",
    "\n",
    "back = 0\n",
    "plt.figure(figsize=(16,500))\n",
    "for i in range(len(label[200:400])):\n",
    "    plt.subplot(100,4,i+1)\n",
    "    if i % 4 == 0:\n",
    "        plt.title(f\"{(i+1+back)/4}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "        \n",
    "    path = label['dir'][i] + '/' + label['image_id'][i]\n",
    "    try:\n",
    "    # im_bgr = cv2.imread(path)\n",
    "    # im_rgb = im_bgr[:, :, ::-1]\n",
    "        temp = Image.open(path).convert(\"RGB\")\n",
    "        image = np.array(temp).copy()\n",
    "        temp.close()\n",
    "\n",
    "        plt.imshow(image, cmap=plt.cm.binary)\n",
    "        plt.xlabel(label['image_id'][i], loc='left', fontsize=10)\n",
    "    except:\n",
    "        plt.xlabel(path, loc='left', fontsize=10)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949b731-608b-43b9-93fa-c4ef70c28a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
