{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c4ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, magic, shutil\n",
    "from glob import glob\n",
    "import time, datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import datetime as dt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, gc\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "#from skimage import io\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, confusion_matrix, classification_report\n",
    "from sklearn import metrics, preprocessing\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import wandb\n",
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d0796-1f1d-40df-ad4a-21f57ed7fe35",
   "metadata": {},
   "source": [
    "#### Hyper Param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c0283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 42,\n",
    "    't_model': 'inception_resnet_v2',\n",
    "    's_model': 'mobilenetv2_100',\n",
    "    'img_size': 260,\n",
    "    'alpha': 0.3,\n",
    "    'epochs': 200,\n",
    "    'train_bs':128,\n",
    "    'valid_bs':32,\n",
    "    'lr': 1e-4, ## learning rate\n",
    "    'num_workers': 8,\n",
    "    'verbose_step': 1,\n",
    "    'patience' : 3,\n",
    "    'device': 'cuda:0',\n",
    "    'freezing': False,\n",
    "    'trainable_layer': 6,\n",
    "    'model_path': './models'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066a8a1-7060-4b6d-bf0c-d4164820a414",
   "metadata": {},
   "source": [
    "#### wandb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127461e4-c16d-47fd-b983-556c12ad0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = dt.datetime.now()\n",
    "run_id = time_now.strftime(\"%Y%m%d%H%M%S\")\n",
    "project_name = 'KD_test_' + run_id\n",
    "user = 'hojunking'\n",
    "run_name = 'KD_test_'+ run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c538155-d55f-4321-bf7c-171d356ebceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 10kwalking\n",
      "img_paths len : 2690\n",
      "\n",
      "label: public_vehicle\n",
      "img_paths len : 1725\n",
      "\n",
      "label: stair\n",
      "img_paths len : 2160\n",
      "\n",
      "label: pet\n",
      "img_paths len : 4335\n",
      "\n",
      "label: outlet\n",
      "img_paths len : 2027\n",
      "\n",
      "label: else\n",
      "img_paths len : 22000\n",
      "\n",
      "label: can\n",
      "img_paths len : 6516\n",
      "\n",
      "label: box\n",
      "img_paths len : 5198\n",
      "\n",
      "label: milk\n",
      "img_paths len : 1399\n",
      "\n",
      "label: cup\n",
      "img_paths len : 1235\n",
      "\n",
      "Train_Images:  14311\n",
      "Train_Images_labels: 14311\n",
      "Test_Images:  4235\n",
      "Test_Images_labels: 4235\n"
     ]
    }
   ],
   "source": [
    "main_path = '../Data/carbon_reduction/'\n",
    "label_list = [\"10kwalking\",\"public_vehicle\",'stair','pet','outlet','else', 'can', 'box', 'milk', 'cup']\n",
    "\n",
    "total_train_img_paths = []\n",
    "total_train_img_labels = []\n",
    "total_test_img_paths = []\n",
    "total_test_img_labels = []\n",
    "\n",
    "for label in label_list: ## 각 레이블 돌기\n",
    "    print(f'label: {label}')\n",
    "    img_paths = [] \n",
    "    img_labels = []\n",
    "\n",
    "    # default ratio\n",
    "    train_ratio = 1500\n",
    "    test_ratio = 500\n",
    "\n",
    "    dir_path = main_path + label ## 레이블 폴더 경로\n",
    "    count = 0\n",
    "    for folder, subfolders, filenames in os.walk(dir_path): ## 폴더 내 모든 파일 탐색\n",
    "    \n",
    "        for img in filenames: ## 각 파일 경로, 레이블 저장\n",
    "            count +=1\n",
    "            if count > train_ratio + test_ratio + 20000:\n",
    "                break\n",
    "            \n",
    "            img_paths.append(folder+'/'+img)\n",
    "            img_labels.append(label)\n",
    "        \n",
    "    random.shuffle(img_paths)\n",
    "    print(f'img_paths len : {len(img_paths)}\\n')\n",
    "\n",
    "    if label == 'milk': ## 10walking 데이터 비율 설정하기 (데이터수: 2494)\n",
    "        train_ratio = 1099\n",
    "        test_ratio = 300\n",
    "    elif label == 'public_vehicle': ## 10walking 데이터 비율 설정하기 (데이터수: 2494)\n",
    "        train_ratio = 1425\n",
    "        test_ratio = 300\n",
    "    elif label == 'cup': ## 10walking 데이터 비율 설정하기 (데이터수: 2494)\n",
    "        train_ratio = 1000\n",
    "        test_ratio = 235\n",
    "    elif label == 'stair': ## 10walking 데이터 비율 설정하기 (데이터수: 2494)\n",
    "        train_ratio = 1660\n",
    "        test_ratio = 500\n",
    "    elif label == 'outlet': ## 10walking 데이터 비율 설정하기 (데이터수: 2494)\n",
    "        train_ratio = 1627\n",
    "        test_ratio = 400\n",
    "    \n",
    "\n",
    "    total_train_img_paths.extend(img_paths[:train_ratio])\n",
    "    total_train_img_labels.extend(img_labels[:train_ratio])\n",
    "\n",
    "    total_test_img_paths.extend(img_paths[-test_ratio:])\n",
    "    total_test_img_labels.extend(img_labels[-test_ratio:])\n",
    "\n",
    "print('Train_Images: ',len(total_train_img_paths))\n",
    "print(\"Train_Images_labels:\", len(total_train_img_labels))\n",
    "print('Test_Images: ',len(total_test_img_paths))\n",
    "print(\"Test_Images_labels:\", len(total_test_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2716dd8b-84db-4707-80e2-19b29eae9c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>else17223.jpg</td>\n",
       "      <td>../Data/carbon_reduction/10kwalking/10kwalking...</td>\n",
       "      <td>10kwalking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>캐시워크 만보_289.jpg</td>\n",
       "      <td>../Data/carbon_reduction/10kwalking/10kwalking...</td>\n",
       "      <td>10kwalking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>만보인증_081.jpg</td>\n",
       "      <td>../Data/carbon_reduction/10kwalking/10kwalking...</td>\n",
       "      <td>10kwalking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>만보인증_182.jpg</td>\n",
       "      <td>../Data/carbon_reduction/10kwalking/10kwalking...</td>\n",
       "      <td>10kwalking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애플워치 만보_390.jpg</td>\n",
       "      <td>../Data/carbon_reduction/10kwalking/10kwalking...</td>\n",
       "      <td>10kwalking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14306</th>\n",
       "      <td>양치컵_080.jpg</td>\n",
       "      <td>../Data/carbon_reduction/cup/양치컵_네이버</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14307</th>\n",
       "      <td>양치컵 사용_272.jpg</td>\n",
       "      <td>../Data/carbon_reduction/cup/양치컵 사용</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14308</th>\n",
       "      <td>양치컵_구글_293.jpg</td>\n",
       "      <td>../Data/carbon_reduction/cup/양치컵_구글</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14309</th>\n",
       "      <td>else31961.jpg</td>\n",
       "      <td>../Data/carbon_reduction/cup/cups_from_else</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14310</th>\n",
       "      <td>양치컵_294.jpg</td>\n",
       "      <td>../Data/carbon_reduction/cup/양치컵_네이버</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14311 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_id                                                dir  \\\n",
       "0        else17223.jpg  ../Data/carbon_reduction/10kwalking/10kwalking...   \n",
       "1      캐시워크 만보_289.jpg  ../Data/carbon_reduction/10kwalking/10kwalking...   \n",
       "2         만보인증_081.jpg  ../Data/carbon_reduction/10kwalking/10kwalking...   \n",
       "3         만보인증_182.jpg  ../Data/carbon_reduction/10kwalking/10kwalking...   \n",
       "4      애플워치 만보_390.jpg  ../Data/carbon_reduction/10kwalking/10kwalking...   \n",
       "...                ...                                                ...   \n",
       "14306      양치컵_080.jpg               ../Data/carbon_reduction/cup/양치컵_네이버   \n",
       "14307   양치컵 사용_272.jpg                ../Data/carbon_reduction/cup/양치컵 사용   \n",
       "14308   양치컵_구글_293.jpg                ../Data/carbon_reduction/cup/양치컵_구글   \n",
       "14309    else31961.jpg        ../Data/carbon_reduction/cup/cups_from_else   \n",
       "14310      양치컵_294.jpg               ../Data/carbon_reduction/cup/양치컵_네이버   \n",
       "\n",
       "            label  \n",
       "0      10kwalking  \n",
       "1      10kwalking  \n",
       "2      10kwalking  \n",
       "3      10kwalking  \n",
       "4      10kwalking  \n",
       "...           ...  \n",
       "14306         cup  \n",
       "14307         cup  \n",
       "14308         cup  \n",
       "14309         cup  \n",
       "14310         cup  \n",
       "\n",
       "[14311 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas 데이터프레임 만들기\n",
    "trn_df = pd.DataFrame(total_train_img_paths, columns=['image_id'])\n",
    "trn_df['dir'] = trn_df['image_id'].apply(lambda x: os.path.dirname(x))\n",
    "trn_df['image_id'] = trn_df['image_id'].apply(lambda x: os.path.basename(x))\n",
    "trn_df['label'] = total_train_img_labels\n",
    "train = trn_df\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c67d39-7933-4f98-b998-54a6036ff4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a8ac38-0b0f-4db4-80d2-6504916b7723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c454bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d97a3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path, sub_path=None):\n",
    "    try:\n",
    "        im_bgr = cv2.imread(path)\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "        past_path = path\n",
    "    except: ## 이미지 에러 발생 시 백지로 대체\n",
    "        im_bgr = cv2.imread('../Data/carbon_reduction/temp_img.jpg')\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884eb987-4341-4fe7-8094-6e61cb051af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose(\n",
    "    [\n",
    "        A.Resize(height = CFG['img_size']+100, width = CFG['img_size']+100),\n",
    "        A.CenterCrop(always_apply=True, p=1.0, height=CFG['img_size'], width=CFG['img_size']),\n",
    "        A.RandomBrightnessContrast(always_apply=False, p=0.8, brightness_limit=(0.00, 0.00), contrast_limit=(0.2, 0.2), brightness_by_max=False),\n",
    "        A.SafeRotate(always_apply=False, p=0.5, limit=(-20, 20), interpolation=2, border_mode=0, value=(0, 0, 0), mask_value=None),\n",
    "        A.HorizontalFlip(always_apply=False, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "transform_test = A.Compose(\n",
    "    [\n",
    "        A.Resize(height = CFG['img_size'], width = CFG['img_size']),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1561be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColonDataset(Dataset):\n",
    "    def __init__(self, df, data_root, transform=None, output_label=True):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transform = transform\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "        \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "        \n",
    "        path = \"{}/{}\".format(self.data_root[index], self.df.iloc[index]['image_id'])\n",
    "        img  = get_img(path)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img)\n",
    "            img = transformed['image']\n",
    "                \n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1601bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForcepImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class=2, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained, num_classes=n_class)\n",
    "        # n_features = self.model.classifier.in_features\n",
    "        # self.model.classifier = nn.Linear(n_features, n_class)\n",
    "    def freezing(self, freeze=False, trainable_layer = 2):\n",
    "        \n",
    "        if freeze:\n",
    "            num_layers = len(list(model.parameters()))\n",
    "            for i, param in enumerate(model.parameters()):\n",
    "                if i < num_layers - trainable_layer*2:\n",
    "                    param.requires_grad = False    \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "240e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root=train.dir.values):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "    train_data_root = data_root[trn_idx]\n",
    "    valid_data_root = data_root[val_idx]\n",
    "    \n",
    "        \n",
    "    train_ds = ColonDataset(train_,\n",
    "                            train_data_root,\n",
    "                            transform=transform_train,\n",
    "                            output_label=True)\n",
    "    valid_ds = ColonDataset(valid_,\n",
    "                            valid_data_root,\n",
    "                            transform=transform_test,\n",
    "                            output_label=True)\n",
    "    \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=True,        \n",
    "        num_workers=CFG['num_workers']\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67bc652d-dfff-45d6-8412-38db1fe187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_loss(student_logits, labels, teacher_logits, criterion, alpha=0.1):\n",
    "    # TEACHER & STUDENT LOSS\n",
    "    distillation_loss = criterion(student_logits, teacher_logits)\n",
    "    \n",
    "    # STUDENT & LABEL LOSS\n",
    "    student_loss = criterion(student_logits, labels)\n",
    "    loss_b = alpha * student_loss + (1-alpha) * distillation_loss\n",
    "\n",
    "    return loss_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ffa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, s_model, t_model, loss_tr, optimizer, train_loader, device, scheduler=None, alpha =0.1):\n",
    "    t = time.time()\n",
    "\n",
    "    # SET MODEL TRAINING MODE\n",
    "    s_model.train()\n",
    "    t_model.eval()\n",
    "    \n",
    "    running_loss = None\n",
    "    loss_sum = 0\n",
    "    student_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # STUDENT MODEL PREDICTION\n",
    "        with torch.cuda.amp.autocast():\n",
    "            student_preds = s_model(imgs)\n",
    "            \n",
    "            # TEACHER MODEL DISTILLATION (NO UPDATE)\n",
    "            with torch.no_grad():\n",
    "                teacher_preds = t_model(imgs)\n",
    "            \n",
    "            loss = distill_loss(student_preds, image_labels, teacher_preds, loss_tr, alpha)\n",
    "            loss_sum+=loss.detach()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01    \n",
    "        \n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                pbar.set_description(description)\n",
    "        \n",
    "        student_preds_all += [torch.argmax(student_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    student_preds_all = np.concatenate(student_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    \n",
    "    matrix = confusion_matrix(image_targets_all,student_preds_all)\n",
    "    epoch_f1 = f1_score(image_targets_all, student_preds_all, average='macro')\n",
    "    \n",
    "    accuracy = (student_preds_all==image_targets_all).mean()\n",
    "    \n",
    "    trn_loss = loss_sum/len(train_loader)\n",
    "    \n",
    "    return image_preds_all, accuracy, trn_loss, matrix, epoch_f1\n",
    "\n",
    "def valid_one_epoch(epoch,s_model, t_model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False, alpha =0.1):\n",
    "    ## Sets the model to valid mode\n",
    "    s_model.eval()\n",
    "    t_model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    avg_loss = 0\n",
    "    image_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        image_preds = s_model(imgs)   #output = model(input)\n",
    "        teacher_output = t_model(imgs) # teacher prediction\n",
    "        \n",
    "        loss = distill_loss(image_preds, image_labels, teacher_output, loss_fn, alpha)\n",
    "        \n",
    "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]\n",
    "        \n",
    "        description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    matrix = confusion_matrix(image_targets_all,image_preds_all)\n",
    "    \n",
    "    epoch_f1 = f1_score(image_targets_all, image_preds_all, average='macro')\n",
    "    acc = (image_preds_all==image_targets_all).mean()\n",
    "    val_loss = avg_loss/len(val_loader)\n",
    "    \n",
    "    return image_preds_all, acc, val_loss, matrix, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e4b92-23e3-4444-9369-1dc9dc504b13",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e9950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best F1 score from now: {self.best_score}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37102a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojunking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hojun/git/KD_models/wandb/run-20230420_144627-3vjpxavb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hojunking/KD_test_20230420144614/runs/3vjpxavb\" target=\"_blank\">clean-elevator-1</a></strong> to <a href=\"https://wandb.ai/hojunking/KD_test_20230420144614\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mobilenetv2_100\n",
      "Training start with fold: 0 epoch: 200 \n",
      "\n",
      "Fold: 0\n",
      "\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/90 [00:00<?, ?it/s]Corrupt JPEG data: 111 extraneous bytes before marker 0xd9\n",
      "epoch 0 loss: -22.7541:  29%|█████▍             | 26/90 [00:25<00:33,  1.93it/s]Premature end of JPEG file\n",
      "epoch 0 loss: -242.0457: 100%|██████████████████| 90/90 [01:01<00:00,  1.46it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_preds_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# TRAINIG\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m train_preds_all, train_acc, train_loss, train_matrix, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m:train_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m : train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain F1\u001b[39m\u001b[38;5;124m'\u001b[39m: train_f1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m : epoch})\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# VALIDATION\u001b[39;00m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch, s_model, t_model, loss_tr, optimizer, train_loader, device, scheduler, alpha)\u001b[0m\n\u001b[1;32m     58\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (student_preds_all\u001b[38;5;241m==\u001b[39mimage_targets_all)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     60\u001b[0m trn_loss \u001b[38;5;241m=\u001b[39m loss_sum\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage_preds_all\u001b[49m, accuracy, trn_loss, matrix, epoch_f1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_preds_all' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    # WANDB TRACKER INIT\n",
    "    wandb.init(project=project_name, entity=user)\n",
    "    wandb.config.update(CFG)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.define_metric(\"Train Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train-Valid Accuracy\", step_metric=\"epoch\")\n",
    "    \n",
    "    model_dir = CFG['model_path'] + '/{}_{}'.format(CFG['s_model'], run_id)\n",
    "    train_dir = train.dir.values\n",
    "    best_fold = 0\n",
    "    \n",
    "    model_name = CFG['s_model']\n",
    "    print(f'Model: {model_name}')\n",
    "    \n",
    "    # MAKE MODEL DIR\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    \n",
    "    # STRATIFIED K-FOLD DEFINITION\n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        break\n",
    "    print(f'Training start with fold: {fold} epoch: {CFG[\"epochs\"]} \\n')\n",
    "    \n",
    "    # EARLY STOPPING DEFINITION\n",
    "    early_stopping = EarlyStopping(patience=CFG[\"patience\"], verbose=True)\n",
    "    \n",
    "    # DATALOADER DEFINITION\n",
    "    train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root=train_dir)\n",
    "\n",
    "    # MODEL & DEVICE DEFINITION \n",
    "    device = torch.device(CFG['device'])\n",
    "    student_model = ForcepImgClassifier(CFG['s_model'], train.label.nunique(), pretrained=True)\n",
    "    teacher_model = ForcepImgClassifier(CFG['t_model'], train.label.nunique(), pretrained=True)\n",
    "    \n",
    "    \n",
    "    # T_MODEL DATA PARALLEL\n",
    "    teacher_model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        teacher_model = nn.DataParallel(teacher_model)\n",
    "    \n",
    "    # LOAD TEACHER_MODE WEIGHT\n",
    "    load_model = CFG['model_path'] +'/' + CFG['t_model'] + '/' + CFG['t_model']\n",
    "    teacher_model.load_state_dict(torch.load(load_model))\n",
    "\n",
    "    # MODEL FREEZING\n",
    "    student_model.freezing(freeze = CFG['freezing'], trainable_layer = CFG['trainable_layer'])\n",
    "    if CFG['freezing'] ==True:\n",
    "        for name, param in student_model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "    # S_MODEL DATA PARALLEL\n",
    "    student_model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        student_model = nn.DataParallel(student_model)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()   \n",
    "    optimizer = torch.optim.Adam(student_model.parameters(), lr=CFG['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=5)\n",
    "\n",
    "    # DISTILLATION RATE\n",
    "    alpha = CFG['alpha']\n",
    "\n",
    "    # CRITERION (LOSS FUNCTION)\n",
    "    loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    wandb.watch(student_model, loss_tr, log='all')\n",
    "    \n",
    "    train_acc_list = []\n",
    "    train_matrix_list = []\n",
    "    train_f1_list = []\n",
    "    valid_acc_list = []\n",
    "    valid_matrix_list = []\n",
    "    valid_f1_list = []\n",
    "    best_f1 =0.0\n",
    "\n",
    "    start = time.time()\n",
    "    print(f'Fold: {fold}\\n')\n",
    "    for epoch in range(CFG['epochs']):\n",
    "        print('Epoch {}/{}'.format(epoch, CFG['epochs'] - 1))\n",
    "        \n",
    "        # TRAINIG\n",
    "        train_preds_all, train_acc, train_loss, train_matrix, train_f1 = train_one_epoch(epoch, student_model, teacher_model,loss_tr, optimizer, train_loader, device, scheduler=scheduler, alpha = alpha)\n",
    "        wandb.log({'Train Accuracy':train_acc, 'Train Loss' : train_loss, 'Train F1': train_f1, 'epoch' : epoch})\n",
    "        \n",
    "        # VALIDATION\n",
    "        with torch.no_grad():\n",
    "            valid_preds_all, valid_acc, valid_loss, valid_matrix, valid_f1= valid_one_epoch(epoch, student_model, teacher_model, loss_fn, val_loader, device, scheduler=None, alpha = alpha)\n",
    "            wandb.log({'Valid Accuracy':valid_acc, 'Valid Loss' : valid_loss, 'Valid F1': valid_f1 ,'epoch' : epoch})\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{train_loss :.5f}] Val Loss : [{val_loss :.5f}] Val F1 Score : [{val_score:.5f}]')\n",
    "        \n",
    "        # SAVE ALL RESULTS\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_matrix_list.append(train_matrix)\n",
    "        train_f1_list.append(train_f1)\n",
    "\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        valid_matrix_list.append(valid_matrix)\n",
    "        valid_f1_list.append(valid_f1)\n",
    "\n",
    "        # MODEL SAVE (THE BEST MODEL OF ALL OF FOLD PROCESS)\n",
    "        if valid_f1 > best_f1:\n",
    "            best_f1 = valid_f1\n",
    "            torch.save(student_model.state_dict(), (model_dir+'/{}').format(CFG['s_model']))\n",
    "\n",
    "        # EARLY STOPPING\n",
    "        stop = early_stopping(valid_f1)\n",
    "        if stop:\n",
    "            print(\"stop called\")   \n",
    "            break\n",
    "\n",
    "    end = time.time() - start\n",
    "    time_ = str(datetime.timedelta(seconds=end)).split(\".\")[0]\n",
    "    print(\"time :\", time_)\n",
    "    \n",
    "    # PRINT BEST F1 SCORE MODEL OF FOLD\n",
    "    best_index = valid_f1_list.index(max(valid_f1_list))\n",
    "    print(f'fold: {fold}, Best Epoch : {best_index}/ {len(valid_f1_list)}')\n",
    "    print(f'Best Train Marco F1 : {train_f1_list[best_index]:.5f}')\n",
    "    print(train_matrix_list[best_index])\n",
    "    print(f'Best Valid Marco F1 : {valid_f1_list[best_index]:.5f}')\n",
    "    print(valid_matrix_list[best_index])\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    \n",
    "    ## K-FOLD END\n",
    "    if valid_f1_list[best_index] > best_fold:\n",
    "        best_fold = valid_f1_list[best_index]\n",
    "        top_fold = fold\n",
    "    print(f'Best Fold F1 score: {best_fold} Top fold : {top_fold}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
