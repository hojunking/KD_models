{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c4ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, magic, shutil\n",
    "from glob import glob\n",
    "import time, datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import datetime as dt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, gc\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "#from skimage import io\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, confusion_matrix, classification_report\n",
    "from sklearn import metrics, preprocessing\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import wandb\n",
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d0796-1f1d-40df-ad4a-21f57ed7fe35",
   "metadata": {},
   "source": [
    "#### Hyper Param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c0283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 42,\n",
    "    't_model': 'inception_resnet_v2',\n",
    "    'load_model': 'inception_resnet_v2_20230421010009', # LOAD TEACHER MODEL\n",
    "    's_model': 'mobilenetv2_100',\n",
    "    'img_size': 260,\n",
    "    'alpha': 0.5,\n",
    "    'epochs': 200,\n",
    "    'train_bs':64,\n",
    "    'valid_bs':32,\n",
    "    'lr': 1e-4, ## learning rate\n",
    "    'num_workers': 8,\n",
    "    'verbose_step': 1,\n",
    "    'patience' : 3,\n",
    "    'device': 'cuda:0',\n",
    "    'freezing': False,\n",
    "    'trainable_layer': 6,\n",
    "    'model_path': './models'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066a8a1-7060-4b6d-bf0c-d4164820a414",
   "metadata": {},
   "source": [
    "#### wandb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127461e4-c16d-47fd-b983-556c12ad0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_now = dt.datetime.now()\n",
    "run_id = time_now.strftime(\"%Y%m%d%H%M%S\")\n",
    "project_name = 'KD_test_' + run_id\n",
    "user = 'hojunking'\n",
    "run_name = 'KD_test_'+ run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c538155-d55f-4321-bf7c-171d356ebceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 10Kwalk\n",
      "img_paths len : 1953\n",
      "\n",
      "label: battery\n",
      "img_paths len : 2800\n",
      "\n",
      "label: box\n",
      "img_paths len : 1987\n",
      "\n",
      "label: else\n",
      "img_paths len : 229\n",
      "\n",
      "label: green dish\n",
      "img_paths len : 1682\n",
      "\n",
      "label: handkerchief\n",
      "img_paths len : 2433\n",
      "\n",
      "label: leftover\n",
      "img_paths len : 1978\n",
      "\n",
      "label: milk\n",
      "img_paths len : 2374\n",
      "\n",
      "label: paper\n",
      "img_paths len : 1665\n",
      "\n",
      "label: pet\n",
      "img_paths len : 2402\n",
      "\n",
      "label: plug\n",
      "img_paths len : 2925\n",
      "\n",
      "label: receipt\n",
      "img_paths len : 843\n",
      "\n",
      "label: shopping bag\n",
      "img_paths len : 1713\n",
      "\n",
      "label: stairs\n",
      "img_paths len : 4057\n",
      "\n",
      "label: toothcup\n",
      "img_paths len : 2803\n",
      "\n",
      "label: transportation\n",
      "img_paths len : 2235\n",
      "\n",
      "label: trash picking\n",
      "img_paths len : 1627\n",
      "\n",
      "label: tumbler\n",
      "img_paths len : 3345\n",
      "\n",
      "label: wrap\n",
      "img_paths len : 1689\n",
      "\n",
      "Train_Images:  2850\n",
      "Train_Images_labels: 2850\n",
      "Test_Images:  950\n",
      "Test_Images_labels: 950\n"
     ]
    }
   ],
   "source": [
    "main_path = '../Data/carbon_data/'\n",
    "label_list = [\"10Kwalk\",\"battery\",'box','else','green dish','handkerchief', 'leftover',\n",
    "              'milk', 'paper', 'pet','plug','receipt', 'shopping bag', 'stairs', 'toothcup',\n",
    "             'transportation', 'trash picking', 'tumbler', 'wrap']\n",
    "\n",
    "total_train_img_paths = []\n",
    "total_train_img_labels = []\n",
    "total_test_img_paths = []\n",
    "total_test_img_labels = []\n",
    "\n",
    "for label in label_list: ## 각 레이블 돌기\n",
    "    print(f'label: {label}')\n",
    "    img_paths = [] \n",
    "    img_labels = []\n",
    "\n",
    "    # default ratio\n",
    "    train_ratio = 150\n",
    "    test_ratio = 50\n",
    "\n",
    "    dir_path = main_path + label ## 레이블 폴더 경로\n",
    "    count = 0\n",
    "    for folder, subfolders, filenames in os.walk(dir_path): ## 폴더 내 모든 파일 탐색\n",
    "    \n",
    "        for img in filenames: ## 각 파일 경로, 레이블 저장\n",
    "            count +=1\n",
    "            if count > train_ratio + test_ratio + 10000:\n",
    "                break\n",
    "            \n",
    "            img_paths.append(folder+'/'+img)\n",
    "            img_labels.append(label)\n",
    "        \n",
    "    random.shuffle(img_paths)\n",
    "    print(f'img_paths len : {len(img_paths)}\\n')\n",
    "\n",
    "    # if label == 'else': ## 10walking 데이터 비율 설정하기 (데이터수: 2494)\n",
    "    #     train_ratio = 200\n",
    "    #     test_ratio = 29\n",
    "    # elif label == 'green dish': \n",
    "    #     train_ratio = 1392\n",
    "    #     test_ratio = 300\n",
    "    # elif label == 'handkerchief':\n",
    "    #     train_ratio = 2000\n",
    "    #     test_ratio = 433     \n",
    "    # elif label == 'milk':\n",
    "    #     train_ratio = 2000\n",
    "    #     test_ratio = 374\n",
    "    # elif label == 'paper':\n",
    "    #     train_ratio = 1300\n",
    "    #     test_ratio = 365\n",
    "    # elif label == 'pet':\n",
    "    #     train_ratio = 2000\n",
    "    #     test_ratio = 402\n",
    "    # elif label == 'plug':\n",
    "    #     train_ratio = 2200\n",
    "    #     test_ratio = 725\n",
    "    # elif label == 'receipt':\n",
    "    #     train_ratio = 600\n",
    "    #     test_ratio = 243 \n",
    "    # elif label == 'shopping bag':\n",
    "    #     train_ratio = 1300\n",
    "    #     test_ratio = 413\n",
    "    # elif label == 'stairs':\n",
    "    #     train_ratio = 3000\n",
    "    #     test_ratio = 1057\n",
    "    # elif label == 'toothcup':\n",
    "    #     train_ratio = 2200\n",
    "    #     test_ratio = 603\n",
    "    # elif label == 'paper':\n",
    "    #     train_ratio = 1300\n",
    "    #     test_ratio = 365\n",
    "    # elif label == 'transportation':\n",
    "    #     train_ratio = 1800\n",
    "    #     test_ratio = 435\n",
    "    # elif label == 'trash picking':\n",
    "    #     train_ratio = 1300\n",
    "    #     test_ratio = 327\n",
    "    # elif label == 'tumbler':\n",
    "    #     train_ratio = 2500\n",
    "    #     test_ratio = 845\n",
    "    # elif label == 'wrap':\n",
    "    #     train_ratio = 1300\n",
    "    #     test_ratio = 389\n",
    "        \n",
    "    total_train_img_paths.extend(img_paths[:train_ratio])\n",
    "    total_train_img_labels.extend(img_labels[:train_ratio])\n",
    "\n",
    "    total_test_img_paths.extend(img_paths[-test_ratio:])\n",
    "    total_test_img_labels.extend(img_labels[-test_ratio:])\n",
    "\n",
    "print('Train_Images: ',len(total_train_img_paths))\n",
    "print(\"Train_Images_labels:\", len(total_train_img_labels))\n",
    "print('Test_Images: ',len(total_test_img_paths))\n",
    "print(\"Test_Images_labels:\", len(total_test_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2716dd8b-84db-4707-80e2-19b29eae9c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10Kwalk_1504.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10Kwalk_1142.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10Kwalk_237.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10Kwalk_520.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10Kwalk_77.jpg</td>\n",
       "      <td>../Data/carbon_data/10Kwalk</td>\n",
       "      <td>10Kwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>wrap_139.jpg</td>\n",
       "      <td>../Data/carbon_data/wrap</td>\n",
       "      <td>wrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>wrap_659.jpg</td>\n",
       "      <td>../Data/carbon_data/wrap</td>\n",
       "      <td>wrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>wrap_469.jpg</td>\n",
       "      <td>../Data/carbon_data/wrap</td>\n",
       "      <td>wrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>wrap_39.jpg</td>\n",
       "      <td>../Data/carbon_data/wrap</td>\n",
       "      <td>wrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>wrap_629.jpg</td>\n",
       "      <td>../Data/carbon_data/wrap</td>\n",
       "      <td>wrap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2850 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_id                          dir    label\n",
       "0     10Kwalk_1504.jpg  ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "1     10Kwalk_1142.jpg  ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "2      10Kwalk_237.jpg  ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "3      10Kwalk_520.jpg  ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "4       10Kwalk_77.jpg  ../Data/carbon_data/10Kwalk  10Kwalk\n",
       "...                ...                          ...      ...\n",
       "2845      wrap_139.jpg     ../Data/carbon_data/wrap     wrap\n",
       "2846      wrap_659.jpg     ../Data/carbon_data/wrap     wrap\n",
       "2847      wrap_469.jpg     ../Data/carbon_data/wrap     wrap\n",
       "2848       wrap_39.jpg     ../Data/carbon_data/wrap     wrap\n",
       "2849      wrap_629.jpg     ../Data/carbon_data/wrap     wrap\n",
       "\n",
       "[2850 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas 데이터프레임 만들기\n",
    "trn_df = pd.DataFrame(total_train_img_paths, columns=['image_id'])\n",
    "trn_df['dir'] = trn_df['image_id'].apply(lambda x: os.path.dirname(x))\n",
    "trn_df['image_id'] = trn_df['image_id'].apply(lambda x: os.path.basename(x))\n",
    "trn_df['label'] = total_train_img_labels\n",
    "train = trn_df\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c67d39-7933-4f98-b998-54a6036ff4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a8ac38-0b0f-4db4-80d2-6504916b7723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 18, 18, 18])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c454bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d97a3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path, sub_path=None):\n",
    "    try:\n",
    "        im_bgr = cv2.imread(path)\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "        past_path = path\n",
    "    except: ## 이미지 에러 발생 시 백지로 대체\n",
    "        im_bgr = cv2.imread('../Data/carbon_reduction/temp_img.jpg')\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884eb987-4341-4fe7-8094-6e61cb051af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.75),ratio=(0.90, 1.10)),\n",
    "        A.SafeRotate(p=0.5, limit=(-20, 20), interpolation=2, border_mode=0, value=(0, 0, 0), mask_value=None),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(always_apply=True, p=0.5, contrast=0.2, saturation=0.3, hue=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "transform_train_cap = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.85),ratio=(0.90, 1.10)),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "transform_test = A.Compose(\n",
    "    [\n",
    "        A.Resize(height = CFG['img_size'], width = CFG['img_size']),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1561be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColonDataset(Dataset):\n",
    "    def __init__(self, df, data_root, transform=None, transform2=None, output_label=True):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transform = transform\n",
    "        self.transform2 = transform2\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "        \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "        \n",
    "        # EXEPTION TRANSFORM FOR CAPTURE IMAGES\n",
    "        self.cap_image = le.fit_transform(['10Kwalk', 'battery','receipt'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        # GET labels\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "        # GET IMAGES\n",
    "        path = \"{}/{}\".format(self.data_root[index], self.df.iloc[index]['image_id'])\n",
    "        img  = get_img(path)\n",
    "        \n",
    "        # TRANSFORM1, TRANSFORM2 PROCESS\n",
    "        if self.transform:\n",
    "            if target in self.cap_image and self.transform2:\n",
    "                transformed = self.transform2(image=img)\n",
    "            else:\n",
    "                transformed = self.transform(image=img)\n",
    "            img = transformed['image']\n",
    "                \n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1601bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForcepImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class=2, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained, num_classes=n_class)\n",
    "        # n_features = self.model.classifier.in_features\n",
    "        # self.model.classifier = nn.Linear(n_features, n_class)\n",
    "    def freezing(self, freeze=False, trainable_layer = 2):\n",
    "        \n",
    "        if freeze:\n",
    "            num_layers = len(list(model.parameters()))\n",
    "            for i, param in enumerate(model.parameters()):\n",
    "                if i < num_layers - trainable_layer*2:\n",
    "                    param.requires_grad = False    \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root=train.dir.values):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "    train_data_root = data_root[trn_idx]\n",
    "    valid_data_root = data_root[val_idx]\n",
    "    \n",
    "        \n",
    "    train_ds = ColonDataset(train_,\n",
    "                            train_data_root,\n",
    "                            transform=transform_train,\n",
    "                            transform2=transform_train_cap,\n",
    "                            output_label=True)\n",
    "    valid_ds = ColonDataset(valid_,\n",
    "                            valid_data_root,\n",
    "                            transform=transform_test,\n",
    "                            output_label=True)\n",
    "    # WEIGHTEDRANDOMSAMPLER\n",
    "    class_counts = train_.label.value_counts(sort=False).to_dict()\n",
    "    num_samples = sum(class_counts.values())\n",
    "    print(f'cls_cnts: {len(class_counts)}\\nnum_samples:{num_samples}')\n",
    "    \n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "    t_labels = train_.label.to_list()\n",
    "    \n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))]\n",
    "\n",
    "\n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "\n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))] \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        sampler=sampler, \n",
    "        num_workers=CFG['num_workers']\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67bc652d-dfff-45d6-8412-38db1fe187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distill_loss(student_logits, labels, teacher_logits, criterion, alpha=0.1):\n",
    "#     # TEACHER & STUDENT LOSS\n",
    "#     distillation_loss = criterion(student_logits, teacher_logits)\n",
    "    \n",
    "#     # STUDENT & LABEL LOSS\n",
    "#     student_loss = criterion(student_logits, labels)\n",
    "#     loss_b = alpha * student_loss + (1-alpha) * distillation_loss\n",
    "\n",
    "#     return loss_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0a7d551-5565-4c29-8398-371af4ece4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_loss(student_logits, labels, teacher_logits, criterion, alpha=0.1, temperature=2):\n",
    "    # STUDENT & LABEL LOSS\n",
    "    student_loss = criterion(student_logits, labels)\n",
    "\n",
    "    # TEACHER & STUDENT LOSS\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    student_probs = F.softmax(student_logits / temperature, dim=1)\n",
    "    distillation_loss = F.kl_div(torch.log(student_probs), teacher_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
    "\n",
    "    # FINAL LOSS\n",
    "    loss_b = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "    return loss_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ffa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, s_model, t_model, loss_tr, optimizer, train_loader, device, scheduler=None, alpha =0.1):\n",
    "    t = time.time()\n",
    "\n",
    "    # SET MODEL TRAINING MODE\n",
    "    s_model.train()\n",
    "    t_model.eval()\n",
    "    \n",
    "    running_loss = None\n",
    "    loss_sum = 0\n",
    "    student_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # STUDENT MODEL PREDICTION\n",
    "        with torch.cuda.amp.autocast():\n",
    "            student_preds = s_model(imgs)\n",
    "            \n",
    "            # TEACHER MODEL DISTILLATION (NO UPDATE)\n",
    "            with torch.no_grad():\n",
    "                teacher_preds = t_model(imgs)\n",
    "            \n",
    "            loss = distill_loss(student_preds, image_labels, teacher_preds, loss_tr, alpha)\n",
    "            loss_sum+=loss.detach()\n",
    "            \n",
    "            # BACKPROPAGATION\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01    \n",
    "        \n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                pbar.set_description(description)\n",
    "        \n",
    "        student_preds_all += [torch.argmax(student_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    student_preds_all = np.concatenate(student_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    \n",
    "    matrix = confusion_matrix(image_targets_all,student_preds_all)\n",
    "    epoch_f1 = f1_score(image_targets_all, student_preds_all, average='macro')\n",
    "    \n",
    "    accuracy = (student_preds_all==image_targets_all).mean()\n",
    "    \n",
    "    trn_loss = loss_sum/len(train_loader)\n",
    "    \n",
    "    return student_preds_all, accuracy, trn_loss, matrix, epoch_f1\n",
    "\n",
    "def valid_one_epoch(epoch,s_model, t_model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False, alpha =0.1):\n",
    "    ## Sets the model to valid mode\n",
    "    s_model.eval()\n",
    "    t_model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    avg_loss = 0\n",
    "    student_preds_all = []\n",
    "    image_targets_all = []\n",
    "    \n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        student_preds = s_model(imgs)   #output = model(input)\n",
    "        teacher_preds = t_model(imgs) # teacher prediction\n",
    "        \n",
    "        loss = distill_loss(student_preds, image_labels, teacher_preds, loss_fn, alpha)\n",
    "        \n",
    "        student_preds_all += [torch.argmax(student_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]\n",
    "        \n",
    "        description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    student_preds_all = np.concatenate(student_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    matrix = confusion_matrix(image_targets_all,student_preds_all)\n",
    "    \n",
    "    epoch_f1 = f1_score(image_targets_all, student_preds_all, average='macro')\n",
    "    acc = (student_preds_all==image_targets_all).mean()\n",
    "    val_loss = avg_loss/len(val_loader)\n",
    "    \n",
    "    return student_preds_all, acc, val_loss, matrix, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e4b92-23e3-4444-9369-1dc9dc504b13",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e9950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best F1 score from now: {self.best_score}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37102a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojunking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hojun/git/KD_models/wandb/run-20230421_115835-1xw740gm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hojunking/KD_test_20230421115830/runs/1xw740gm\" target=\"_blank\">soft-galaxy-1</a></strong> to <a href=\"https://wandb.ai/hojunking/KD_test_20230421115830\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mobilenetv2_100\n",
      "Training start with fold: 0 epoch: 200 \n",
      "\n",
      "cls_cnts: 19\n",
      "num_samples:2280\n",
      "Fold: 0\n",
      "\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 6.5727: 100%|█████████████████████| 18/18 [00:57<00:00,  3.18s/it]\n",
      "epoch 0 loss: 4.4202: 100%|█████████████████████| 18/18 [00:16<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [5.73288] Val Loss : [4.41256] Val F1 Score : [0.28393]\n",
      "Epoch 1/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 loss: 4.1432: 100%|█████████████████████| 18/18 [00:50<00:00,  2.79s/it]\n",
      "epoch 1 loss: 3.1437: 100%|█████████████████████| 18/18 [00:15<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [3.77295] Val Loss : [3.14232] Val F1 Score : [0.51979]\n",
      "Epoch 2/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 2 loss: 2.9839: 100%|█████████████████████| 18/18 [00:52<00:00,  2.90s/it]\n",
      "epoch 2 loss: 2.2618: 100%|█████████████████████| 18/18 [00:16<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [2.59937] Val Loss : [2.26317] Val F1 Score : [0.66465]\n",
      "Epoch 3/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3 loss: 2.0384: 100%|█████████████████████| 18/18 [00:50<00:00,  2.81s/it]\n",
      "epoch 3 loss: 1.8181: 100%|█████████████████████| 18/18 [00:15<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [1.91012] Val Loss : [1.82185] Val F1 Score : [0.71417]\n",
      "Epoch 4/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 4 loss: 1.5832: 100%|█████████████████████| 18/18 [00:50<00:00,  2.82s/it]\n",
      "epoch 4 loss: 1.5645: 100%|█████████████████████| 18/18 [00:15<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [1.47887] Val Loss : [1.56839] Val F1 Score : [0.75885]\n",
      "Epoch 5/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5 loss: 1.3509: 100%|█████████████████████| 18/18 [00:51<00:00,  2.85s/it]\n",
      "epoch 5 loss: 1.4740: 100%|█████████████████████| 18/18 [00:16<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [1.31063] Val Loss : [1.47788] Val F1 Score : [0.77150]\n",
      "Epoch 6/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 6 loss: 1.2289: 100%|█████████████████████| 18/18 [00:52<00:00,  2.91s/it]\n",
      "epoch 6 loss: 1.4054: 100%|█████████████████████| 18/18 [00:15<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [1.18195] Val Loss : [1.40846] Val F1 Score : [0.78060]\n",
      "Epoch 7/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7 loss: 0.9575: 100%|█████████████████████| 18/18 [00:51<00:00,  2.86s/it]\n",
      "epoch 7 loss: 1.3471: 100%|█████████████████████| 18/18 [00:16<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [1.08248] Val Loss : [1.35142] Val F1 Score : [0.80468]\n",
      "Epoch 8/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 8 loss: 1.0062: 100%|█████████████████████| 18/18 [00:49<00:00,  2.72s/it]\n",
      "epoch 8 loss: 1.2908: 100%|█████████████████████| 18/18 [00:15<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [1.10285] Val Loss : [1.29682] Val F1 Score : [0.80442]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.8046837434266739\n",
      "Epoch 9/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 9 loss: 0.9165: 100%|█████████████████████| 18/18 [00:49<00:00,  2.76s/it]\n",
      "epoch 9 loss: 1.2579: 100%|█████████████████████| 18/18 [00:16<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.99356] Val Loss : [1.26255] Val F1 Score : [0.81876]\n",
      "Epoch 10/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 10 loss: 0.9284: 100%|████████████████████| 18/18 [00:53<00:00,  2.99s/it]\n",
      "epoch 10 loss: 1.2428: 100%|████████████████████| 18/18 [00:16<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [0.98238] Val Loss : [1.24762] Val F1 Score : [0.80690]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.818757097392439\n",
      "Epoch 11/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 11 loss: 0.9045: 100%|████████████████████| 18/18 [00:50<00:00,  2.80s/it]\n",
      "epoch 11 loss: 1.2240: 100%|████████████████████| 18/18 [00:16<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [0.91656] Val Loss : [1.22937] Val F1 Score : [0.81554]\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Best F1 score from now: 0.818757097392439\n",
      "Epoch 12/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 12 loss: 0.8792: 100%|████████████████████| 18/18 [00:52<00:00,  2.92s/it]\n",
      "epoch 12 loss: 1.2186: 100%|████████████████████| 18/18 [00:16<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [0.89057] Val Loss : [1.22276] Val F1 Score : [0.82060]\n",
      "Epoch 13/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 13 loss: 0.7353: 100%|████████████████████| 18/18 [00:48<00:00,  2.71s/it]\n",
      "epoch 13 loss: 1.1971: 100%|████████████████████| 18/18 [00:15<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [0.87112] Val Loss : [1.20070] Val F1 Score : [0.82237]\n",
      "Epoch 14/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 14 loss: 0.7226: 100%|████████████████████| 18/18 [00:51<00:00,  2.87s/it]\n",
      "epoch 14 loss: 1.1841: 100%|████████████████████| 18/18 [00:16<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [0.83957] Val Loss : [1.18796] Val F1 Score : [0.82171]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.8223681916104675\n",
      "Epoch 15/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 15 loss: 0.7911: 100%|████████████████████| 18/18 [00:52<00:00,  2.92s/it]\n",
      "epoch 15 loss: 1.1843: 100%|████████████████████| 18/18 [00:16<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Train Loss : [0.83346] Val Loss : [1.18873] Val F1 Score : [0.83113]\n",
      "Epoch 16/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 16 loss: 0.9000: 100%|████████████████████| 18/18 [00:52<00:00,  2.94s/it]\n",
      "epoch 16 loss: 1.1717: 100%|████████████████████| 18/18 [00:16<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Train Loss : [0.80561] Val Loss : [1.17630] Val F1 Score : [0.83053]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.8311319565110892\n",
      "Epoch 17/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 17 loss: 0.9027: 100%|████████████████████| 18/18 [00:50<00:00,  2.80s/it]\n",
      "epoch 17 loss: 1.1715: 100%|████████████████████| 18/18 [00:15<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], Train Loss : [0.85927] Val Loss : [1.17587] Val F1 Score : [0.83223]\n",
      "Epoch 18/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 18 loss: 0.7573: 100%|████████████████████| 18/18 [00:50<00:00,  2.83s/it]\n",
      "epoch 18 loss: 1.1514: 100%|████████████████████| 18/18 [00:17<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Train Loss : [0.79021] Val Loss : [1.15536] Val F1 Score : [0.82475]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.832233892784856\n",
      "Epoch 19/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 19 loss: 0.7036: 100%|████████████████████| 18/18 [00:55<00:00,  3.06s/it]\n",
      "epoch 19 loss: 1.1472: 100%|████████████████████| 18/18 [00:16<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19], Train Loss : [0.79207] Val Loss : [1.15171] Val F1 Score : [0.83366]\n",
      "Epoch 20/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 20 loss: 0.8847: 100%|████████████████████| 18/18 [00:51<00:00,  2.86s/it]\n",
      "epoch 20 loss: 1.1416: 100%|████████████████████| 18/18 [00:15<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], Train Loss : [0.84771] Val Loss : [1.14556] Val F1 Score : [0.83007]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.8336583094542044\n",
      "Epoch 21/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 21 loss: 0.7923: 100%|████████████████████| 18/18 [00:49<00:00,  2.78s/it]\n",
      "epoch 21 loss: 1.1442: 100%|████████████████████| 18/18 [00:15<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21], Train Loss : [0.78223] Val Loss : [1.14865] Val F1 Score : [0.82953]\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Best F1 score from now: 0.8336583094542044\n",
      "Epoch 22/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 22 loss: 0.7324: 100%|████████████████████| 18/18 [00:49<00:00,  2.77s/it]\n",
      "epoch 22 loss: 1.1391: 100%|████████████████████| 18/18 [00:17<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22], Train Loss : [0.77673] Val Loss : [1.14396] Val F1 Score : [0.83427]\n",
      "Epoch 23/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 23 loss: 0.7631: 100%|████████████████████| 18/18 [00:51<00:00,  2.84s/it]\n",
      "epoch 23 loss: 1.1447: 100%|████████████████████| 18/18 [00:16<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23], Train Loss : [0.79908] Val Loss : [1.14924] Val F1 Score : [0.83414]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.8342710143200225\n",
      "Epoch 24/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 24 loss: 0.7131: 100%|████████████████████| 18/18 [00:51<00:00,  2.86s/it]\n",
      "epoch 24 loss: 1.1255: 100%|████████████████████| 18/18 [00:16<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24], Train Loss : [0.78563] Val Loss : [1.13105] Val F1 Score : [0.83439]\n",
      "Epoch 25/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 25 loss: 0.7004: 100%|████████████████████| 18/18 [00:51<00:00,  2.86s/it]\n",
      "epoch 25 loss: 1.1334: 100%|████████████████████| 18/18 [00:15<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25], Train Loss : [0.80007] Val Loss : [1.13827] Val F1 Score : [0.84354]\n",
      "Epoch 26/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 26 loss: 0.7745: 100%|████████████████████| 18/18 [00:51<00:00,  2.87s/it]\n",
      "epoch 26 loss: 1.1372: 100%|████████████████████| 18/18 [00:15<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26], Train Loss : [0.81664] Val Loss : [1.14102] Val F1 Score : [0.83546]\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Best F1 score from now: 0.8435357301388945\n",
      "Epoch 27/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 27 loss: 1.0773: 100%|████████████████████| 18/18 [00:51<00:00,  2.87s/it]\n",
      "epoch 27 loss: 1.1264: 100%|████████████████████| 18/18 [00:16<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27], Train Loss : [0.83271] Val Loss : [1.13045] Val F1 Score : [0.84173]\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Best F1 score from now: 0.8435357301388945\n",
      "Epoch 28/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 28 loss: 0.6396: 100%|████████████████████| 18/18 [00:54<00:00,  3.03s/it]\n",
      "epoch 28 loss: 1.1325: 100%|████████████████████| 18/18 [00:16<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28], Train Loss : [0.80601] Val Loss : [1.13678] Val F1 Score : [0.83175]\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Best F1 score from now: 0.8435357301388945\n",
      "stop called\n",
      "time : 0:33:12\n",
      "fold: 0, Best Epoch : 25/ 29\n",
      "Best Train Marco F1 : 0.89487\n",
      "[[ 96   2   0   0   0   1   1   0   0   0   0   3   0   0   0   2   0   0\n",
      "    0]\n",
      " [  1 126   0   0   0   0   0   0   0   0   0   9   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0 117   4   0   0   0   4   0   1   0   0   0   0   0   0   1   0\n",
      "    0]\n",
      " [  0   2   1  75   1   1   0   2   5   2   1   3   9   1   1   5   2   3\n",
      "    4]\n",
      " [  0   0   0   0 109   3   3   0   0   1   0   0   4   0   1   0   1   0\n",
      "    4]\n",
      " [  0   0   1   1   0  92   1   0   3   1   0   0   9   0   1   0   2   0\n",
      "    2]\n",
      " [  0   0   0   0   2   0 110   0   0   0   0   0   0   0   2   0   0   0\n",
      "    4]\n",
      " [  0   0   2   2   0   0   0 109   2   0   3   0   2   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   1   2   0   0   0   2 111   0   0   3   0   0   0   2   0   0\n",
      "    0]\n",
      " [  0   0   0   1   1   1   1   0   1 103   0   0   1   0   0   1   1   1\n",
      "    1]\n",
      " [  0   0   1   0   0   1   0   0   0   0  97   0   0   1   0   0   0   1\n",
      "    0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0 110   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   1   0   1   4   0   5   0   3   0   0 108   1   1   3   3   0\n",
      "    0]\n",
      " [  0   0   0   0   0   1   0   1   0   0   0   0   0 119   0   3   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   1   1   0   0   2   2   0   1   0 130   1   0   4\n",
      "    0]\n",
      " [  0   0   0   3   0   0   0   1   1   0   0   0   0   2   0  95   1   0\n",
      "    0]\n",
      " [  0   0   1   2   1   0   0   2   0   2   0   0   6   0   0   0 112   3\n",
      "    0]\n",
      " [  0   0   0   3   1   0   0   0   0   1   0   0   0   0   4   0   0 114\n",
      "    2]\n",
      " [  0   0   0   0   0   1   1   0   1   0   2   0   0   1   1   1   0   1\n",
      "  109]]\n",
      "Best Valid Marco F1 : 0.84354\n",
      "[[26  1  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0 25  0  2  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0 25  0  0  0  0  1  1  1  0  0  0  0  0  0  2  0  0]\n",
      " [ 1  0  1 13  0  0  0  1  1  0  3  1  5  0  1  1  1  0  1]\n",
      " [ 0  0  0  0 26  0  2  0  0  0  0  0  0  0  0  0  1  0  1]\n",
      " [ 0  0  0  0  1 27  0  0  0  0  0  0  2  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 25  0  0  0  1  0  0  0  2  0  0  0  2]\n",
      " [ 0  0  1  1  0  0  0 28  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 29  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0 25  0  0  1  0  0  0  2  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 26  0  0  0  1  1  0  1  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 30  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  3  0  0  0  0  0  0 22  0  0  2  3  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1 29  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0 29  0  0  0  0]\n",
      " [ 0  0  0  2  0  0  0  0  0  0  0  0  0  2  0 26  0  0  0]\n",
      " [ 0  0  0  1  0  1  0  1  0  1  0  0  0  1  0  0 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  1  0  0  0  3  2  0 21  2]\n",
      " [ 0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  2  0  1 25]]\n",
      "-----------------------------------------------------------------------\n",
      "Best Fold F1 score: 0.8435357301388945 Top fold : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    # WANDB TRACKER INIT\n",
    "    wandb.init(project=project_name, entity=user)\n",
    "    wandb.config.update(CFG)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.define_metric(\"Train Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train-Valid Accuracy\", step_metric=\"epoch\")\n",
    "    \n",
    "    model_dir = CFG['model_path'] + '/{}_{}'.format(CFG['s_model'], run_id)\n",
    "    train_dir = train.dir.values\n",
    "    best_fold = 0\n",
    "    best_f1 =0.0\n",
    "    \n",
    "    print('Model: {}'.format(CFG['s_model']))\n",
    "    # MAKE MODEL DIR\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    \n",
    "    # STRATIFIED K-FOLD DEFINITION\n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        print(f'Training start with fold: {fold} epoch: {CFG[\"epochs\"]} \\n')\n",
    "\n",
    "        # EARLY STOPPING DEFINITION\n",
    "        early_stopping = EarlyStopping(patience=CFG[\"patience\"], verbose=True)\n",
    "\n",
    "        # DATALOADER DEFINITION\n",
    "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root=train_dir)\n",
    "\n",
    "        # MODEL & DEVICE DEFINITION \n",
    "        device = torch.device(CFG['device'])\n",
    "        student_model = ForcepImgClassifier(CFG['s_model'], train.label.nunique(), pretrained=True)\n",
    "        teacher_model = ForcepImgClassifier(CFG['t_model'], train.label.nunique(), pretrained=True)\n",
    "\n",
    "\n",
    "        # T_MODEL DATA PARALLEL\n",
    "        teacher_model.to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            teacher_model = nn.DataParallel(teacher_model)\n",
    "\n",
    "        # LOAD TEACHER_MODE WEIGHT\n",
    "        load_model = CFG['model_path'] +'/' + CFG['load_model'] + '/' + CFG['t_model']\n",
    "        teacher_model.load_state_dict(torch.load(load_model))\n",
    "\n",
    "        # MODEL FREEZING\n",
    "        student_model.freezing(freeze = CFG['freezing'], trainable_layer = CFG['trainable_layer'])\n",
    "        if CFG['freezing'] ==True:\n",
    "            for name, param in student_model.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "        # S_MODEL DATA PARALLEL\n",
    "        student_model.to(device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            student_model = nn.DataParallel(student_model)\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler()   \n",
    "        optimizer = torch.optim.Adam(student_model.parameters(), lr=CFG['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=5)\n",
    "\n",
    "        # DISTILLATION RATE\n",
    "        alpha = CFG['alpha']\n",
    "\n",
    "        # CRITERION (LOSS FUNCTION)\n",
    "        loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "        wandb.watch(student_model, loss_tr, log='all')\n",
    "\n",
    "        train_acc_list = []\n",
    "        train_matrix_list = []\n",
    "        train_f1_list = []\n",
    "        valid_acc_list = []\n",
    "        valid_matrix_list = []\n",
    "        valid_f1_list = []\n",
    "\n",
    "        start = time.time()\n",
    "        print(f'Fold: {fold}\\n')\n",
    "        for epoch in range(CFG['epochs']):\n",
    "            print('Epoch {}/{}'.format(epoch, CFG['epochs'] - 1))\n",
    "\n",
    "            # TRAINIG\n",
    "            train_preds_all, train_acc, train_loss, train_matrix, train_f1 = train_one_epoch(epoch, student_model, teacher_model,loss_tr, optimizer, train_loader, device, scheduler=scheduler, alpha = alpha)\n",
    "            wandb.log({'Train Accuracy':train_acc, 'Train Loss' : train_loss, 'Train F1': train_f1, 'epoch' : epoch})\n",
    "\n",
    "            # VALIDATION\n",
    "            with torch.no_grad():\n",
    "                valid_preds_all, valid_acc, valid_loss, valid_matrix, valid_f1= valid_one_epoch(epoch, student_model, teacher_model, loss_fn, val_loader, device, scheduler=None, alpha = alpha)\n",
    "                wandb.log({'Valid Accuracy':valid_acc, 'Valid Loss' : valid_loss, 'Valid F1': valid_f1 ,'epoch' : epoch})\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{train_loss :.5f}] Val Loss : [{valid_loss :.5f}] Val F1 Score : [{valid_f1:.5f}]')\n",
    "\n",
    "            # SAVE ALL RESULTS\n",
    "            train_acc_list.append(train_acc)\n",
    "            train_matrix_list.append(train_matrix)\n",
    "            train_f1_list.append(train_f1)\n",
    "\n",
    "            valid_acc_list.append(valid_acc)\n",
    "            valid_matrix_list.append(valid_matrix)\n",
    "            valid_f1_list.append(valid_f1)\n",
    "\n",
    "            # MODEL SAVE (THE BEST MODEL OF ALL OF FOLD PROCESS)\n",
    "            if valid_f1 > best_f1:\n",
    "                best_f1 = valid_f1\n",
    "                torch.save(student_model.state_dict(), (model_dir+'/{}').format(CFG['s_model']))\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            stop = early_stopping(valid_f1)\n",
    "            if stop:\n",
    "                print(\"stop called\")   \n",
    "                break\n",
    "\n",
    "        end = time.time() - start\n",
    "        time_ = str(datetime.timedelta(seconds=end)).split(\".\")[0]\n",
    "        print(\"time :\", time_)\n",
    "\n",
    "        # PRINT BEST F1 SCORE MODEL OF FOLD\n",
    "        best_index = valid_f1_list.index(max(valid_f1_list))\n",
    "        print(f'fold: {fold}, Best Epoch : {best_index}/ {len(valid_f1_list)}')\n",
    "        print(f'Best Train Marco F1 : {train_f1_list[best_index]:.5f}')\n",
    "        print(train_matrix_list[best_index])\n",
    "        print(f'Best Valid Marco F1 : {valid_f1_list[best_index]:.5f}')\n",
    "        print(valid_matrix_list[best_index])\n",
    "        print('-----------------------------------------------------------------------')\n",
    "\n",
    "        ## K-FOLD END\n",
    "        if valid_f1_list[best_index] > best_fold:\n",
    "            best_fold = valid_f1_list[best_index]\n",
    "            top_fold = fold\n",
    "    print(f'Best Fold F1 score: {best_fold} Top fold : {top_fold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cd221-5c4f-4255-8f42-7e898c240898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
