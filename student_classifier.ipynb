{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c4ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, magic, shutil\n",
    "from glob import glob\n",
    "import time, datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import datetime as dt\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, gc\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "#from skimage import io\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, confusion_matrix, classification_report\n",
    "from sklearn import metrics, preprocessing\n",
    "from scipy.ndimage import zoom\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d0796-1f1d-40df-ad4a-21f57ed7fe35",
   "metadata": {},
   "source": [
    "#### Hyper Param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c0283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 5,\n",
    "    'seed': 42,\n",
    "    't_model': 'resnet152',\n",
    "    'load_model': 'KD_resnet152_pet_202305202118', # LOAD TEACHER MODEL\n",
    "    's_model': 'mobilenet_v3_large',\n",
    "    'img_size': 260,\n",
    "    'alpha': 0.5, # HARD LABEL : SOFT LABEL RATIO\n",
    "    'epochs': 200, \n",
    "    'train_bs':32,\n",
    "    'valid_bs':32,\n",
    "    'lr': 1e-4, ## learning rate\n",
    "    'num_workers': 8,\n",
    "    'verbose_step': 1,\n",
    "    'patience' : 5,\n",
    "    'label_encoder':False,\n",
    "    'device': 'cuda:0',\n",
    "    'freezing': False,\n",
    "    'trainable_layer': 6,\n",
    "    'model_path': './models'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066a8a1-7060-4b6d-bf0c-d4164820a414",
   "metadata": {},
   "source": [
    "#### wandb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127461e4-c16d-47fd-b983-556c12ad0daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'pet'\n",
    "time_now = dt.datetime.now()\n",
    "run_id = time_now.strftime(\"%Y%m%d%H%M\")\n",
    "project_name = 'KD_'+ category + '_'+ CFG['t_model'] + '_' + CFG['s_model']\n",
    "user = 'hojunking'\n",
    "run_name = project_name + '_' + run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c538155-d55f-4321-bf7c-171d356ebceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: unlabeled 1803\n",
      "label: labeled 3396\n",
      "Train_Images:  5199\n",
      "Train_Images_labels: 5199\n"
     ]
    }
   ],
   "source": [
    "# TRAIN DATASET DATAFRAME\n",
    "train_path = '../Data/carbon_reduction_data/bin/train/'\n",
    "label_list = [\"unlabeled\",\"labeled\"]\n",
    "\n",
    "train_img_paths = []\n",
    "train_img_labels = []\n",
    "\n",
    "for label in label_list: ## 각 레이블 돌기\n",
    "    print(f'label: {label}',end=' ')\n",
    "    img_paths = [] \n",
    "    img_labels = []\n",
    "\n",
    "    dir_path = train_path + label ## 레이블 폴더 경로\n",
    "    \n",
    "    for folder, subfolders, filenames in os.walk(dir_path): ## 폴더 내 모든 파일 탐색\n",
    "        for img in filenames: ## 각 파일 경로, 레이블 저장\n",
    "            img_paths.append(folder+'/'+img)\n",
    "            img_labels.append(label)\n",
    "        \n",
    "    print(len(img_paths))\n",
    "\n",
    "    train_img_paths.extend(img_paths)\n",
    "    train_img_labels.extend(img_labels)\n",
    "\n",
    "print('Train_Images: ',len(train_img_paths))\n",
    "print(\"Train_Images_labels:\", len(train_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f420162-da43-4ac0-9187-d4aedf9533e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: unlabeled 601\n",
      "label: labeled 1133\n",
      "Test_Images:  1734\n",
      "Test_Images_labels: 1734\n"
     ]
    }
   ],
   "source": [
    "# TEST DATASET DATAFRAME\n",
    "test_path = '../Data/carbon_reduction_data/bin/test/'\n",
    "test_img_paths = []\n",
    "test_img_labels = []\n",
    "\n",
    "for label in label_list: ## 각 레이블 돌기\n",
    "    print(f'label: {label}',end=' ')\n",
    "    img_paths = [] \n",
    "    img_labels = []\n",
    "    dir_path = test_path + label ## 레이블 폴더 경로\n",
    "    \n",
    "    for folder, subfolders, filenames in os.walk(dir_path): ## 폴더 내 모든 파일 탐색\n",
    "        for img in filenames: ## 각 파일 경로, 레이블 저장\n",
    "            img_paths.append(folder+'/'+img)\n",
    "            img_labels.append(label)\n",
    "        \n",
    "    print(len(img_paths))\n",
    "\n",
    "    test_img_paths.extend(img_paths)\n",
    "    test_img_labels.extend(img_labels)\n",
    "\n",
    "print('Test_Images: ',len(test_img_paths))\n",
    "print(\"Test_Images_labels:\", len(test_img_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2716dd8b-84db-4707-80e2-19b29eae9c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0720.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1028.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1322.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>2893.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>1512.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>2070.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>2728.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5199 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                                dir      label\n",
       "0     0720.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "1     0282.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "2     1028.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "3     1322.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "4     0540.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled  unlabeled\n",
       "...        ...                                                ...        ...\n",
       "5194  2893.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5195  1059.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5196  1512.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5197  2070.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "5198  2728.jpg    ../Data/carbon_reduction_data/bin/train/labeled    labeled\n",
       "\n",
       "[5199 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas 데이터프레임 만들기\n",
    "trn_df = pd.DataFrame(train_img_paths, columns=['image_id'])\n",
    "trn_df['dir'] = trn_df['image_id'].apply(lambda x: os.path.dirname(x))\n",
    "trn_df['image_id'] = trn_df['image_id'].apply(lambda x: os.path.basename(x))\n",
    "trn_df['label'] = train_img_labels\n",
    "train = trn_df\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b84a8c2-bdde-4fae-b68a-964bb3702c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0466.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0190.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0234.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/unlabeled</td>\n",
       "      <td>unlabeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>0011.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>0868.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>0611.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0692.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/test/labeled</td>\n",
       "      <td>labeled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                               dir      label\n",
       "0     0282.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "1     0540.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "2     0466.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "3     0190.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "4     0234.jpg  ../Data/carbon_reduction_data/bin/test/unlabeled  unlabeled\n",
       "...        ...                                               ...        ...\n",
       "1729  0011.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1730  0868.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1731  0611.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1732  0692.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "1733  1059.jpg    ../Data/carbon_reduction_data/bin/test/labeled    labeled\n",
       "\n",
       "[1734 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas 데이터프레임 만들기\n",
    "tst_df = pd.DataFrame(test_img_paths, columns=['image_id'])\n",
    "tst_df['dir'] = tst_df['image_id'].apply(lambda x: os.path.dirname(x))\n",
    "tst_df['image_id'] = tst_df['image_id'].apply(lambda x: os.path.basename(x))\n",
    "tst_df['label'] = test_img_labels\n",
    "test = tst_df\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c67d39-7933-4f98-b998-54a6036ff4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0720.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1028.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1322.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/unlabeled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>2893.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>1059.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>1512.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>2070.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>2728.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/bin/train/labeled</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5199 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                                dir  label\n",
       "0     0720.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "1     0282.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "2     1028.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "3     1322.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "4     0540.jpg  ../Data/carbon_reduction_data/bin/train/unlabeled      1\n",
       "...        ...                                                ...    ...\n",
       "5194  2893.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5195  1059.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5196  1512.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5197  2070.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "5198  2728.jpg    ../Data/carbon_reduction_data/bin/train/labeled      0\n",
       "\n",
       "[5199 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'].values)\n",
    "test['label'] = le.transform(test['label'].values)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3fa5ff-2c3f-4b28-af67-e7ee2533d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding_classes():\n",
    "    # define certain classes to transform differently\n",
    "    capture_image_classes = ['10Kwalk', 'battery','receipt']\n",
    "    return le.transform(capture_image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c454bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97a3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path, sub_path=None):\n",
    "    try:\n",
    "        im_bgr = cv2.imread(path)\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "        past_path = path\n",
    "    except: ## 이미지 에러 발생 시 백지로 대체\n",
    "        im_bgr = cv2.imread('../Data/carbon_reduction/temp_img.jpg')\n",
    "        im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "884eb987-4341-4fe7-8094-6e61cb051af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.Compose([\n",
    "            A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.75),ratio=(0.90, 1.10)),\n",
    "        ], p=0.8),\n",
    "        A.Compose([\n",
    "            A.Resize(p=1, height = CFG['img_size'], width = CFG['img_size']),\n",
    "        ], p=0.2),\n",
    "    ], p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.SafeRotate(p=0.5, limit=(-20, 20), interpolation=2, border_mode=0, value=(0, 0, 0), mask_value=None),\n",
    "    A.ColorJitter(always_apply=True, p=0.5, contrast=0.2, saturation=0.3, hue=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0),\n",
    "    A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "transform_train_cap = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.Compose([\n",
    "            A.RandomResizedCrop(p=1, height=CFG['img_size'] ,width=CFG['img_size'], scale=(0.65, 0.85),ratio=(0.90, 1.10)),\n",
    "        ], p=0.6),\n",
    "        A.Compose([\n",
    "            A.Resize(p=1, height = CFG['img_size'], width = CFG['img_size']),\n",
    "        ], p=0.4),\n",
    "    ], p=1.0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=True, p=1.0),\n",
    "    A.pytorch.transforms.ToTensorV2()\n",
    "])\n",
    "\n",
    "transform_test = A.Compose([\n",
    "    A.Resize(height = CFG['img_size'], width = CFG['img_size']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "    A.pytorch.transforms.ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f1561be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, data_root, transform=None, transform2=None, output_label=True, encoded_class=False):\n",
    "        super(CustomDataset,self).__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transform = transform\n",
    "        self.transform2 = transform2\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "         \n",
    "        if encoded_class == True:\n",
    "            self.encoded_class = label_encoding_classes()\n",
    "        else:\n",
    "            self.encoded_class = encoded_class\n",
    "            \n",
    "        if output_label == True:\n",
    "            self.labels = self.df['label'].values\n",
    "        \n",
    "    # AUGMENTATION DIFFERENTLY DEPENDING ON THE TARGET\n",
    "    def custom_augmentation(self, img, target):\n",
    "        if self.encoded_class is not False and target in self.encoded_class:\n",
    "            return self.transform2(image=img)\n",
    "        else:\n",
    "            return self.transform(image=img)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # GET IMAGES\n",
    "        path = \"{}/{}\".format(self.data_root[index], self.df.iloc[index]['image_id'])\n",
    "        img  = get_img(path)\n",
    "        \n",
    "        # GET LABELS\n",
    "        if self.output_label:\n",
    "            target = self.labels[index]\n",
    "            \n",
    "            # CUSTOM AUGMENTATION\n",
    "            transformed = self.custom_augmentation(img, target) \n",
    "            img = transformed['image']\n",
    "            return img, target\n",
    "        else:\n",
    "            transformed =self.transform(image=img)\n",
    "            img = transformed['image']\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a5b82b-cb1e-4e91-8c70-3a1890bc707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED MODEL\n",
    "class Student(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes= 2,pretrained=True):\n",
    "        super(Student, self).__init__()\n",
    "        self.backbone = models.mobilenet_v3_large(pretrained=pretrained) ## 모델 선언 여기 models.##(pretrained =pretrained)\n",
    "        self.backbone.classifier[-1] = nn.Linear(self.backbone.classifier[-1].in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "572bd9cd-7650-4d6d-8574-7a4942eef620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED MODEL\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self, model_arch, num_classes= 2,pretrained=True):\n",
    "        super(Teacher, self).__init__()\n",
    "        self.backbone = models.resnet152(pretrained=pretrained) ## 모델 선언 여기 models.##(pretrained =pretrained)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(df, trn_idx, val_idx, data_root=train.dir.values):\n",
    "    \n",
    "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
    "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
    "    train_data_root = data_root[trn_idx]\n",
    "    valid_data_root = data_root[val_idx]\n",
    "    \n",
    "        \n",
    "    train_ds = CustomDataset(train_, train_data_root, transform=transform_train,\n",
    "                            transform2=transform_train_cap, output_label=True, encoded_class=CFG['label_encoder'])\n",
    "    valid_ds = CustomDataset(valid_, valid_data_root, transform=transform_test,\n",
    "                            output_label=True)\n",
    "    # WEIGHTEDRANDOMSAMPLER\n",
    "    class_counts = train_.label.value_counts(sort=False).to_dict()\n",
    "    num_samples = sum(class_counts.values())\n",
    "    print(f'cls_cnts: {len(class_counts)}\\nnum_samples:{num_samples}')\n",
    "    \n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "    t_labels = train_.label.to_list()\n",
    "    \n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))]\n",
    "\n",
    "\n",
    "    # weight 제작, 전체 학습 데이터 수를 해당 클래스의 데이터 수로 나누어 줌\n",
    "    class_weights = {l:round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
    "\n",
    "    # class 별 weight를 전체 trainset에 대응시켜 sampler에 넣어줌\n",
    "    weights = [class_weights[t_labels[i]] for i in range(int(num_samples))] \n",
    "    sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CFG['train_bs'],\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        sampler=sampler, \n",
    "        num_workers=CFG['num_workers']\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        valid_ds, \n",
    "        batch_size=CFG['valid_bs'],\n",
    "        num_workers=CFG['num_workers'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67bc652d-dfff-45d6-8412-38db1fe187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distill_loss(student_logits, labels, teacher_logits, criterion, alpha=0.1):\n",
    "#     # TEACHER & STUDENT LOSS\n",
    "#     distillation_loss = criterion(student_logits, teacher_logits)\n",
    "    \n",
    "#     # STUDENT & LABEL LOSS\n",
    "#     student_loss = criterion(student_logits, labels)\n",
    "#     loss_b = alpha * student_loss + (1-alpha) * distillation_loss\n",
    "\n",
    "#     return loss_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a7d551-5565-4c29-8398-371af4ece4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_loss(student_logits, labels, teacher_logits, criterion, alpha=0.1, temperature=2):\n",
    "    # STUDENT & LABEL LOSS\n",
    "    student_loss = criterion(student_logits, labels)\n",
    "\n",
    "    # TEACHER & STUDENT LOSS\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    student_probs = F.softmax(student_logits / temperature, dim=1)\n",
    "    distillation_loss = F.kl_div(torch.log(student_probs), teacher_probs, reduction=\"batchmean\") * (temperature ** 2)\n",
    "\n",
    "    # FINAL LOSS\n",
    "    loss_b = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "    return loss_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08ffa2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, s_model, t_model, loss_tr, optimizer, train_loader, device, scheduler=None, alpha =0.1):\n",
    "    t = time.time()\n",
    "\n",
    "    # SET MODEL TRAINING MODE\n",
    "    s_model.train()\n",
    "    t_model.eval()\n",
    "    \n",
    "    running_loss = None\n",
    "    loss_sum = 0\n",
    "    student_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # STUDENT MODEL PREDICTION\n",
    "        with torch.cuda.amp.autocast():\n",
    "            student_preds = s_model(imgs)\n",
    "            \n",
    "            # TEACHER MODEL DISTILLATION (NO UPDATE)\n",
    "            with torch.no_grad():\n",
    "                teacher_preds = t_model(imgs)\n",
    "            \n",
    "            # DISTILLATION LOSS\n",
    "            loss = distill_loss(student_preds, image_labels, teacher_preds, loss_tr, alpha)\n",
    "            loss_sum+=loss.detach()\n",
    "            \n",
    "            # BACKPROPAGATION\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01    \n",
    "            \n",
    "            # TQDM VERBOSE_STEP TRACKING\n",
    "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
    "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "                pbar.set_description(description)\n",
    "        \n",
    "        student_preds_all += [torch.argmax(student_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    student_preds_all = np.concatenate(student_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    \n",
    "    matrix = confusion_matrix(image_targets_all,student_preds_all)\n",
    "    epoch_f1 = f1_score(image_targets_all, student_preds_all, average='macro')\n",
    "    accuracy = (student_preds_all==image_targets_all).mean()\n",
    "    trn_loss = loss_sum/len(train_loader)\n",
    "    \n",
    "    return student_preds_all, accuracy, trn_loss, matrix, epoch_f1\n",
    "\n",
    "def valid_one_epoch(epoch,s_model, t_model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False, alpha =0.1):\n",
    "    t = time.time()\n",
    "    \n",
    "    # SET MODEL VALID MODE\n",
    "    s_model.eval()\n",
    "    t_model.eval()\n",
    "\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    avg_loss = 0\n",
    "    student_preds_all = []\n",
    "    image_targets_all = []\n",
    "    acc_list = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (imgs, image_labels) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        image_labels = image_labels.to(device).long()\n",
    "        \n",
    "        # STUDENT MODEL PREDICTION\n",
    "        student_preds = s_model(imgs)\n",
    "        # TEACHER MODEL PREDICTION\n",
    "        teacher_preds = t_model(imgs)\n",
    "        \n",
    "        # DISTILLATION LOSS\n",
    "        loss = distill_loss(student_preds, image_labels, teacher_preds, loss_fn, alpha)\n",
    "        \n",
    "        student_preds_all += [torch.argmax(student_preds, 1).detach().cpu().numpy()]\n",
    "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        loss_sum += loss.item()*image_labels.shape[0]\n",
    "        sample_num += image_labels.shape[0]\n",
    "        \n",
    "        # TQDM\n",
    "        description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    student_preds_all = np.concatenate(student_preds_all)\n",
    "    image_targets_all = np.concatenate(image_targets_all)\n",
    "    matrix = confusion_matrix(image_targets_all,student_preds_all)\n",
    "    \n",
    "    epoch_f1 = f1_score(image_targets_all, student_preds_all, average='macro')\n",
    "    acc = (student_preds_all==image_targets_all).mean()\n",
    "    val_loss = avg_loss/len(val_loader)\n",
    "    \n",
    "    return student_preds_all, acc, val_loss, matrix, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e4b92-23e3-4444-9369-1dc9dc504b13",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77e9950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, score):\n",
    "        print(f' present score: {score}')\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            print(f'Best F1 score from now: {self.best_score}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37102a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojunking\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hojun/git/KD_models/wandb/run-20230520_221310-9f2wo58c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hojunking/KD_pet_resnet152_mobilenet_v3_large/runs/9f2wo58c' target=\"_blank\">crimson-snow-1</a></strong> to <a href='https://wandb.ai/hojunking/KD_pet_resnet152_mobilenet_v3_large' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hojunking/KD_pet_resnet152_mobilenet_v3_large' target=\"_blank\">https://wandb.ai/hojunking/KD_pet_resnet152_mobilenet_v3_large</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hojunking/KD_pet_resnet152_mobilenet_v3_large/runs/9f2wo58c' target=\"_blank\">https://wandb.ai/hojunking/KD_pet_resnet152_mobilenet_v3_large/runs/9f2wo58c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mobilenet_v3_large\n",
      "Training start with fold: 0 epoch: 200 \n",
      "\n",
      "cls_cnts: 2\n",
      "num_samples:4159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "\n",
      "Epoch 0/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.6824: 100%|██████████████████████████████████████████████████████████████████████| 130/130 [01:11<00:00,  1.83it/s]\n",
      "epoch 0 loss: 0.0751: 100%|████████████████████████████████████████████████████████████████████████| 33/33 [00:24<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.37673] Val Loss : [0.07419] Val F1 Score : [0.98734]\n",
      " present score: 0.9873363774733638\n",
      "Epoch 1/199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1 loss: 0.0636:  76%|██████████████████████████████████████████████████████                 | 99/130 [00:49<00:15,  1.95it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    # WANDB TRACKER INIT\n",
    "    wandb.init(project=project_name, entity=user)\n",
    "    wandb.config.update(CFG)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.define_metric(\"Train Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Accuracy\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Valid Macro F1 Score\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"Train-Valid Accuracy\", step_metric=\"epoch\")\n",
    "    \n",
    "    model_dir = CFG['model_path'] + '/{}'.format(run_name)\n",
    "    train_dir = train.dir.values\n",
    "    best_fold = 0\n",
    "    best_f1 =0.0\n",
    "    \n",
    "    print('Model: {}'.format(CFG['s_model']))\n",
    "    # MAKE MODEL DIR\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    \n",
    "    # STRATIFIED K-FOLD DEFINITION\n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        print(f'Training start with fold: {fold} epoch: {CFG[\"epochs\"]} \\n')\n",
    "\n",
    "        # EARLY STOPPING DEFINITION\n",
    "        early_stopping = EarlyStopping(patience=CFG[\"patience\"], verbose=True)\n",
    "\n",
    "        # DATALOADER DEFINITION\n",
    "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root=train_dir)\n",
    "\n",
    "        # MODEL & DEVICE DEFINITION \n",
    "        device = torch.device(CFG['device'])\n",
    "        student_model = Student(CFG['s_model'], train.label.nunique(), pretrained=True)\n",
    "        teacher_model = Teacher(CFG['t_model'], train.label.nunique(), pretrained=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # LOAD TEACHER_MODE WEIGHT\n",
    "        load_model = CFG['model_path'] +'/' + CFG['load_model'] + '/' + CFG['t_model'] +'.pth'\n",
    "        teacher_model.load_state_dict(torch.load(load_model))\n",
    "\n",
    "        # MODEL FREEZING\n",
    "        # student_model.freezing(freeze = CFG['freezing'], trainable_layer = CFG['trainable_layer'])\n",
    "        # if CFG['freezing'] ==True:\n",
    "        #     for name, param in student_model.named_parameters():\n",
    "        #         if param.requires_grad == True:\n",
    "        #             print(f\"{name}: {param.requires_grad}\")\n",
    "        \n",
    "        # T_MODEL DATA PARALLEL\n",
    "        # S_MODEL DATA PARALLEL\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            teacher_model = nn.DataParallel(teacher_model)\n",
    "            student_model = nn.DataParallel(student_model)\n",
    "            \n",
    "        teacher_model.to(device)\n",
    "        student_model.to(device)\n",
    "        scaler = torch.cuda.amp.GradScaler()   \n",
    "        optimizer = torch.optim.Adam(student_model.parameters(), lr=CFG['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.5, step_size=5)\n",
    "\n",
    "        # DISTILLATION RATE\n",
    "        alpha = CFG['alpha']\n",
    "\n",
    "        # CRITERION (LOSS FUNCTION)\n",
    "        loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "        \n",
    "        wandb.watch(student_model, loss_tr, log='all')\n",
    "        train_acc_list = []\n",
    "        train_matrix_list = []\n",
    "        train_f1_list = []\n",
    "        valid_acc_list = []\n",
    "        valid_matrix_list = []\n",
    "        valid_f1_list = []\n",
    "\n",
    "        start = time.time()\n",
    "        print(f'Fold: {fold}\\n')\n",
    "        for epoch in range(CFG['epochs']):\n",
    "            print('Epoch {}/{}'.format(epoch, CFG['epochs'] - 1))\n",
    "\n",
    "            # TRAINIG\n",
    "            train_preds_all, train_acc, train_loss, train_matrix, train_f1 = train_one_epoch(epoch, student_model, teacher_model,loss_tr, optimizer, train_loader, device, scheduler=scheduler, alpha = alpha)\n",
    "            wandb.log({'Train Accuracy':train_acc, 'Train Loss' : train_loss, 'Train F1': train_f1, 'epoch' : epoch})\n",
    "\n",
    "            # VALIDATION\n",
    "            with torch.no_grad():\n",
    "                valid_preds_all, valid_acc, valid_loss, valid_matrix, valid_f1= valid_one_epoch(epoch, student_model, teacher_model, loss_fn, val_loader, device, scheduler=None, alpha = alpha)\n",
    "                wandb.log({'Valid Accuracy':valid_acc, 'Valid Loss' : valid_loss, 'Valid F1': valid_f1 ,'epoch' : epoch})\n",
    "            print(f'Epoch [{epoch}], Train Loss : [{train_loss :.5f}] Val Loss : [{valid_loss :.5f}] Val F1 Score : [{valid_f1:.5f}]')\n",
    "\n",
    "            # SAVE ALL RESULTS\n",
    "            train_acc_list.append(train_acc)\n",
    "            train_matrix_list.append(train_matrix)\n",
    "            train_f1_list.append(train_f1)\n",
    "\n",
    "            valid_acc_list.append(valid_acc)\n",
    "            valid_matrix_list.append(valid_matrix)\n",
    "            valid_f1_list.append(valid_f1)\n",
    "\n",
    "            # MODEL SAVE (THE BEST MODEL OF ALL OF FOLD PROCESS)\n",
    "            if valid_f1 > best_f1:\n",
    "                best_f1 = valid_f1\n",
    "                torch.save(student_model.module.state_dict(), (model_dir+'/{}.pth').format(CFG['s_model']))\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            stop = early_stopping(valid_f1)\n",
    "            if stop:\n",
    "                print(\"stop called\")   \n",
    "                break\n",
    "\n",
    "        end = time.time() - start\n",
    "        time_ = str(datetime.timedelta(seconds=end)).split(\".\")[0]\n",
    "        print(\"time :\", time_)\n",
    "\n",
    "        # PRINT BEST F1 SCORE MODEL OF FOLD\n",
    "        best_index = valid_f1_list.index(max(valid_f1_list))\n",
    "        print(f'fold: {fold}, Best Epoch : {best_index}/ {len(valid_f1_list)}')\n",
    "        print(f'Best Train Marco F1 : {train_f1_list[best_index]:.5f}')\n",
    "        print(train_matrix_list[best_index])\n",
    "        print(f'Best Valid Marco F1 : {valid_f1_list[best_index]:.5f}')\n",
    "        print(valid_matrix_list[best_index])\n",
    "        print('-----------------------------------------------------------------------')\n",
    "\n",
    "        ## K-FOLD END\n",
    "        if valid_f1_list[best_index] > best_fold:\n",
    "            best_fold = valid_f1_list[best_index]\n",
    "            top_fold = fold\n",
    "    print(f'Best Fold F1 score: {best_fold} Top fold : {top_fold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a437e36-9c91-4f37-8a6c-f3eadce8fecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/wrap</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0190.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/wrap</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0234.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/wrap</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0392.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/wrap</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0375.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/wrap</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>0381.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/green dish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>0236.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/green dish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>0384.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/green dish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>0074.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/green dish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>0011.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/green dish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                            dir  label\n",
       "0     0282.jpg        ../Data/carbon_reduction_data/test/wrap      2\n",
       "1     0190.jpg        ../Data/carbon_reduction_data/test/wrap      2\n",
       "2     0234.jpg        ../Data/carbon_reduction_data/test/wrap      2\n",
       "3     0392.jpg        ../Data/carbon_reduction_data/test/wrap      2\n",
       "4     0375.jpg        ../Data/carbon_reduction_data/test/wrap      2\n",
       "...        ...                                            ...    ...\n",
       "1334  0381.jpg  ../Data/carbon_reduction_data/test/green dish      0\n",
       "1335  0236.jpg  ../Data/carbon_reduction_data/test/green dish      0\n",
       "1336  0384.jpg  ../Data/carbon_reduction_data/test/green dish      0\n",
       "1337  0074.jpg  ../Data/carbon_reduction_data/test/green dish      0\n",
       "1338  0011.jpg  ../Data/carbon_reduction_data/test/green dish      0\n",
       "\n",
       "[1339 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef05a94-6c5d-4171-8286-8efa23ba7a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## inference #############################\n",
    "def inference(model, data_loader, device):\n",
    "    model.eval()\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "\n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf8dd532-5acf-4d36-93df-55a97860eb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hojun/miniconda3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:42<00:00,  1.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0282.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0540.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0466.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0190.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0234.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/toothcup</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>0074.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>0515.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0011.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>0611.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>0692.jpg</td>\n",
       "      <td>../Data/carbon_reduction_data/test/tumbler</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1539 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                                          dir  label  pred\n",
       "0     0282.jpg  ../Data/carbon_reduction_data/test/toothcup      0     0\n",
       "1     0540.jpg  ../Data/carbon_reduction_data/test/toothcup      0     0\n",
       "2     0466.jpg  ../Data/carbon_reduction_data/test/toothcup      0     0\n",
       "3     0190.jpg  ../Data/carbon_reduction_data/test/toothcup      0     0\n",
       "4     0234.jpg  ../Data/carbon_reduction_data/test/toothcup      0     0\n",
       "...        ...                                          ...    ...   ...\n",
       "1534  0074.jpg   ../Data/carbon_reduction_data/test/tumbler      1     1\n",
       "1535  0515.jpg   ../Data/carbon_reduction_data/test/tumbler      1     1\n",
       "1536  0011.jpg   ../Data/carbon_reduction_data/test/tumbler      1     1\n",
       "1537  0611.jpg   ../Data/carbon_reduction_data/test/tumbler      1     1\n",
       "1538  0692.jpg   ../Data/carbon_reduction_data/test/tumbler      1     1\n",
       "\n",
       "[1539 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN INFERENCE\n",
    "model = Student(CFG['s_model'], test.label.nunique(), pretrained=True)\n",
    "load_model = CFG['model_path'] + '/KD_bottle_resnet152_mobilenet_v3_large_202305201459/' + CFG['s_model'] + '.pth'\n",
    "test_dir = test.dir.values\n",
    "\n",
    "tst_ds = CustomDataset(test, test_dir, transform=transform_test, output_label=False)\n",
    "tst_loader = torch.utils.data.DataLoader(\n",
    "    tst_ds, \n",
    "    batch_size=CFG['train_bs'],\n",
    "    num_workers=CFG['num_workers'],\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "device = torch.device(CFG['device'])\n",
    "\n",
    "# INFERENCE VIA MULTI-GPU\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#         model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# RUN INFERENCE\n",
    "predictions = []\n",
    "model.load_state_dict(torch.load(load_model))\n",
    "with torch.no_grad():\n",
    "    predictions += [inference(model, tst_loader, device)]\n",
    "\n",
    "\n",
    "predictions = np.mean(predictions, axis=0) \n",
    "test['pred'] = np.argmax(predictions, axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63e86c0e-080e-4a45-ba54-20db2560660d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: ['toothcup' 'tumbler']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      3\u001b[0m test\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:161\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    159\u001b[0m diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(y, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)))\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n\u001b[1;32m    162\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[y]\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: ['toothcup' 'tumbler']"
     ]
    }
   ],
   "source": [
    "test['label'] = le.inverse_transform(test['label'].values)\n",
    "test['pred'] = le.inverse_transform(test['pred'].values)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c6af9cf-1048-40ed-a983-d831cc5ac39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9864\n",
      "f1_score: 0.9862\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABE4AAANCCAYAAAB4WYl5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXOklEQVR4nO3deZyd89k/8M/JNkksIYksiCSWIEgQRXgQax+UR7UVPBWxFbXHGvvaoKoUoUosVUvVUtVYUg2l9kjttWssIZLUWs025/eHn3k6JjeZnInJHO+313m9zPfc97mvMzPtjM9c1/0tlcvlcgAAAABooFVzFwAAAACwsBKcAAAAABQQnAAAAAAUEJwAAAAAFBCcAAAAABQQnAAAAAAUEJwAAAAAFBCcAAAAABQQnAAAAAAUEJwAfANceeWVKZVKad++ff7xj380eH7IkCFZffXVm6GypjF8+PD06dOn3lqfPn0yfPjwr7WO119/PaVSKVdeeeU8Hf/qq6/mwAMPTL9+/dKhQ4d07Ngxq622Wo4//vi89dZbC7zWbbfdNp07d06pVMqhhx7a5Ndojq9Bktx7770plUpf+rXYbLPNUiqVGnzfzKtrr7025513XqPOaez3BwCwcGjT3AUA8PWZMWNGjj/++Pz6179u7lIWuFtuuSWLL754c5dR6Pbbb8/OO++crl275sADD8xaa62VUqmUp59+OmPGjMkf//jHTJw4cYFd/7DDDssjjzySMWPGpEePHunZs2eTX6O5vwaLLbZYLr/88gbhzWuvvZZ77723otquvfbaPPPMM40KnHr27JmHHnooK6ywwnxfFwD4+glOAL5B/vu//zvXXnttjjjiiAwcOHCBXefTTz9Nhw4dFtjrz4u11lqrWa//ZV577bXsvPPO6devX8aPH59OnTrVPbfZZpvl4IMPzi233LJAa3jmmWey7rrrZocddlhg12jur8HQoUNz2WWX5aWXXspKK61Utz5mzJgss8wyWWONNfLcc88t8DrmzJmT2bNnp6amJuuvv/4Cvx4A0LSM6gB8gxx11FHp0qVLjj766K889t///ndGjhyZvn37pl27dllmmWVywAEH5P333693XJ8+ffKd73wnN998c9Zaa620b98+p5xySt24xLXXXpujjz46PXv2zKKLLprtttsu7777bj766KP86Ec/SteuXdO1a9fsscce+fjjj+u99kUXXZSNN9443bp1yyKLLJI11lgjZ599dmbNmvWV9X9xTGTIkCF14xtffPzn6MQ777yTfffdN8suu2zatWuXvn375pRTTsns2bPrvf7bb7+dnXbaKYsttlg6deqUoUOH5p133vnKupLk3HPPzSeffJLRo0fXC00+VyqVsuOOO9ZbGzNmTAYOHJj27dunc+fO+e53v5vnn3++3jHDhw/PoosumpdffjnbbLNNFl100fTq1SuHH354ZsyYkeT/xlhefvnl3HHHHXWfg9dff71upOv111+v97qfn3PvvffWrU2cODHf+c530q1bt9TU1GTppZfOtttumzfffLPumLmN6kyaNCk//OEP685bddVV87Of/Sy1tbV1x3w+0nLOOefk3HPPTd++fbPoootm8ODBefjhh+fpc5wkW265ZXr16pUxY8bUrdXW1uaqq67K7rvvnlatGv4aNC/fc0OGDMkf//jH/OMf/6j3ffSftZ999tk5/fTT07dv39TU1GT8+PENRnX+/e9/Z6211sqKK66YDz74oO7133nnnfTo0SNDhgzJnDlz5vn9AgALho4TgG+QxRZbLMcff3wOOeSQ/PnPf85mm2021+PK5XJ22GGH3HPPPRk5cmQ22mijPPXUUznppJPy0EMP5aGHHkpNTU3d8U888USef/75HH/88enbt28WWWSRfPLJJ0mSY489NptuummuvPLKvP766zniiCOyyy67pE2bNhk4cGCuu+66TJw4Mccee2wWW2yx/OIXv6h73VdeeSW77rprXXjz5JNP5owzzsjf//73ev8xPC9Gjx6dDz/8sN7aCSeckPHjx2fllVdO8tl/sK677rpp1apVTjzxxKywwgp56KGHcvrpp+f111/PFVdckeSzjpotttgib7/9dkaNGpV+/frlj3/8Y4YOHTpPtdx9993p3r37PHcfjBo1Kscee2x22WWXjBo1KtOmTcvJJ5+cwYMH57HHHqvXTTFr1qxsv/322WuvvXL44YfnL3/5S0477bR06tQpJ554YtZee+089NBD+e53v5sVVlgh55xzTpI0alTnk08+yZZbbpm+ffvmoosuSvfu3fPOO+9k/Pjx+eijjwrPe++997LBBhtk5syZOe2009KnT5/cfvvtOeKII/LKK69k9OjR9Y6/6KKLssoqq9TdS+SEE07INttsk9dee22ugdMXtWrVKsOHD8/ll1+e008/Pa1bt87dd9+dN998M3vssUcOOeSQBufMy/fc6NGj86Mf/SivvPJKYWfQL37xi/Tr1y/nnHNOFl988Xpfo8+1b98+v/3tbzNo0KDsueeeuemmm1JbW5v//d//TblcznXXXZfWrVt/5fsEABawMgBV74orrignKT/22GPlGTNmlJdffvnyOuusU66trS2Xy+XyJptsUl5ttdXqjr/zzjvLScpnn312vde54YYbyknKl156ad1a7969y61bty6/8MIL9Y4dP358OUl5u+22q7d+6KGHlpOUDz744HrrO+ywQ7lz586F72HOnDnlWbNmla+++upy69aty9OnT697bvfddy/37t273vG9e/cu77777oWv99Of/rTBe9l3333Liy66aPkf//hHvWPPOeeccpLys88+Wy6Xy+WLL764nKT8+9//vt5x++yzTzlJ+Yorrii8brlcLrdv3768/vrrf+kxn/vnP/9Z7tChQ3mbbbaptz5p0qRyTU1Nedddd61b23333ctJyr/97W/rHbvNNtuUV1555XprvXv3Lm+77bb11j7/PnnttdfqrX/+tRw/fny5XC6XH3/88XKS8q233vqltX/xa3DMMceUk5QfeeSResftv//+5VKpVPc99Nprr5WTlNdYY43y7Nmz64579NFHy0nK11133Zde9/N6b7zxxvKrr75aLpVK5dtvv71cLpfLP/jBD8pDhgwpl8vl8rbbbtvg++Y/fdn3XNG5n9e+wgorlGfOnDnX5774/fH5/67OO++88oknnlhu1apV+e677/7S9wgAfH2M6gB8w7Rr1y6nn356Hn/88fz2t7+d6zF//vOfk6TBmMUPfvCDLLLIIrnnnnvqrQ8YMCD9+vWb62t95zvfqffxqquumiTZdtttG6xPnz693rjOxIkTs/3226dLly5p3bp12rZtm2HDhmXOnDl58cUXv/rNFrjuuuty1FFH5fjjj88+++xTt3777bdn0003zdJLL53Zs2fXPbbeeuskyX333ZckGT9+fBZbbLFsv/329V531113ne+aijz00EP59NNPG3wtevXqlc0226zB16JUKmW77bartzZgwIC57qY0v1ZcccUsueSSOfroo3PJJZfM831C/vznP6d///5Zd911660PHz485XK57vvuc9tuu229josBAwYkSaPeS9++fTNkyJCMGTMm06ZNy+9///vsueeehcc31ffc9ttvn7Zt287TsTvttFP233//HHnkkTn99NNz7LHHZsstt5znawEAC5bgBOAbaOedd87aa6+d4447bq73C5k2bVratGmTpZZaqt56qVRKjx49Mm3atHrrXzbm0blz53oft2vX7kvX//3vfyf57F4YG220Ud56662cf/75uf/++/PYY4/loosuSvLZuMz8GD9+fIYPH55hw4bltNNOq/fcu+++mz/84Q9p27Ztvcdqq62WJJk6dWqSzz4/3bt3b/DaPXr0mKcalltuubz22mvzdOznn+u5fY6XXnrpBl+Ljh07pn379vXWampq6j6vTaFTp0657777suaaa+bYY4/NaqutlqWXXjonnXTSl95/Ztq0aYXv4/Pn/1OXLl3qffz5eFhjv/Z77bVX/vCHP+Tcc89Nhw4d8v3vf3+uxzXl91xjdynac889M2vWrLRp0yYHH3xwo84FABYs9zgB+AYqlUo566yzsuWWW+bSSy9t8HyXLl0ye/bsvPfee/XCk3K5nHfeeSff+ta3GrxeU7v11lvzySef5Oabb07v3r3r1v/2t7/N92s+9dRT2WGHHbLJJpvkV7/6VYPnu3btmgEDBuSMM86Y6/mf/wd+ly5d8uijjzZ4fl5vDvvtb387F1xwQR5++OGvvM/J5+HB5MmTGzz39ttvp2vXrvN0zXnxeeDy+Y1kP/d5YPSf1lhjjVx//fUpl8t56qmncuWVV+bUU09Nhw4dcswxx8z19bt06VL4PpI06Xv5TzvuuGMOOOCAnHnmmdlnn30Kd3xqyu+5xvxv4pNPPsluu+2Wfv365d13383ee++d3//+942+JgCwYOg4AfiG2mKLLbLlllvm1FNPbbCbzeabb54kueaaa+qt33TTTfnkk0/qnl+QPv8Pz/+8CW25XJ5r4DEvJk2alK233jrLL798brrpprmOUXznO9/JM888kxVWWCHrrLNOg8fnwcmmm26ajz76KLfddlu986+99tp5quWwww7LIosskh//+Mf1dlP5XLlcrrvp6ODBg9OhQ4cGX4s333wzf/7zn5v0a9GnT58knwVM/+mL7/M/lUqlDBw4MD//+c+zxBJL5Iknnig8dvPNN89zzz3X4Jirr746pVIpm2666fwX/yU6dOiQE088Mdttt13233//wuMa8z1XU1Mz311PX7Tffvtl0qRJufnmm3P55Zfntttuy89//vMmeW0AoHI6TgC+wc4666wMGjQoU6ZMqRtHST7bxvXb3/52jj766Hz44YfZcMMN63bVWWuttbLbbrst8Nq23HLLtGvXLrvsskuOOuqo/Pvf/87FF1+cf/7zn/P1eltvvXXef//9XHjhhXn22WfrPbfCCitkqaWWyqmnnppx48Zlgw02yMEHH5yVV145//73v/P6669n7NixueSSS7Lssstm2LBh+fnPf55hw4bljDPOyEorrZSxY8fmrrvumqda+vbtm+uvvz5Dhw7NmmuumQMPPDBrrbVWkuS5557LmDFjUi6X893vfjdLLLFETjjhhBx77LEZNmxYdtlll0ybNi2nnHJK2rdvn5NOOmm+Ph9z861vfSsrr7xyjjjiiMyePTtLLrlkbrnlljzwwAP1jrv99tszevTo7LDDDll++eVTLpdz88035/333//Se3Mcdthhufrqq7Ptttvm1FNPTe/evfPHP/4xo0ePzv777194n5ymMGLEiIwYMeJLj2nM99waa6yRm2++ORdffHEGDRqUVq1aZZ111ml0XZdddlmuueaaXHHFFVlttdWy2mqr5cADD8zRRx+dDTfcsMH9YACAr5/gBOAbbK211souu+zSoFOiVCrl1ltvzcknn5wrrrgiZ5xxRrp27ZrddtstP/nJT+r9RX5BWWWVVXLTTTfl+OOPz4477pguXbpk1113zYgRI+pu1toYn9/AdMcdd2zw3BVXXJHhw4enZ8+eefzxx3Paaaflpz/9ad58880stthi6du3b/77v/87Sy65ZJLP7iPy5z//OYccckiOOeaYlEqlbLXVVrn++uuzwQYbzFM93/nOd/L000/nZz/7WS655JK88cYbadWqVd21DjrooLpjR44cmW7duuUXv/hFbrjhhnTo0CFDhgzJT37yk7luczu/WrdunT/84Q858MADs99++6WmpiY777xzLrzwwno3811ppZWyxBJL5Oyzz87bb7+ddu3aZeWVV86VV16Z3XffvfD1l1pqqTz44IMZOXJkRo4cmQ8//DDLL798zj777K8MNb4OjfmeO+SQQ/Lss8/m2GOPzQcffJByuZxyudyo6z399NM5+OCDs/vuu9e7+e8555yThx56KEOHDs3EiROzxBJLNMG7AwDmV6nc2J/yAAAAAN8Q7nECAAAAUEBwAgAAAFBAcAIAAABQQHACAAAAUEBwAgAAAFBAcAIAAABQQHACAAAAUKBNcxfwuVlTXmruEgCgReq47JDmLgEAWqRZM99q7hK+FrOmvtrcJcxV267LN3cJ80THCQAAAEABwQkAAABAgYVmVAcAAABYAGrnNHcFLZqOEwAAAIACghMAAACAAkZ1AAAAoJqVa5u7ghZNxwkAAABAAcEJAAAAQAGjOgAAAFDNao3qVELHCQAAAEABwQkAAABAAaM6AAAAUMXKdtWpiI4TAAAAgAKCEwAAAIACRnUAAACgmtlVpyI6TgAAAAAKCE4AAAAAChjVAQAAgGpmV52K6DgBAAAAKCA4AQAAAChgVAcAAACqWe2c5q6gRdNxAgAAAFBAcAIAAABQwKgOAAAAVDO76lRExwkAAABAAcEJAAAAQAGjOgAAAFDNao3qVELHCQAAAEABwQkAAABAAaM6AAAAUMXKdtWpiI4TAAAAgAKCEwAAAIACRnUAAACgmtlVpyI6TgAAAAAKCE4AAAAAChjVAQAAgGpmV52K6DgBAAAAKCA4AQAAAChgVAcAAACqWe2c5q6gRdNxAgAAAFBAcAIAAABQwKgOAAAAVDO76lRExwkAAABAAcEJAAAAQAGjOgAAAFDNao3qVELHCQAAAEABwQkAAABAAaM6AAAAUM3sqlMRHScAAAAABQQnAAAAAAWM6gAAAEA1s6tORXScAAAAABQQnAAAAAAUMKoDAAAAVaxcntPcJbRoOk4AAAAACghOAAAAAAoY1QEAAIBqVrarTiV0nAAAAAAUEJwAAAAAFDCqAwAAANWs1qhOJXScAAAAABQQnAAAAAAUMKoDAAAA1cyuOhXRcQIAAABQQHACAAAAUMCoDgAAAFSz2jnNXUGLpuMEAAAAoIDgBAAAAKCAUR0AAACoZnbVqYiOEwAAAIACghMAAACAAkZ1AAAAoJrVGtWphI4TAAAAgAKCEwAAAIACRnUAAACgmtlVpyI6TgAAAAAKCE4AAAAAChjVAQAAgGpmV52K6DgBAAAAKCA4AQAAAChgVAcAAACqmVGdiug4AQAAACggOAEAAAAoYFQHAAAAqli5PKe5S2jRdJwAAAAAFBCcAAAAABQQnAAAAAAUcI8TAAAAqGa2I66IjhMAAACAAoITAAAAgAJGdQAAAKCalY3qVELHCQAAAEABwQkAAABAAaM6AAAAUM3sqlMRHScAAAAABQQnAAAAAAWM6gAAAEA1s6tORXScAAAAABQQnAAAAAAUMKoDAAAA1cyuOhXRcQIAAABQQHACAAAAUMCoDgAAAFQzu+pURMcJAAAAQAHBCQAAAEABozoAAABQzeyqUxEdJwAAAAAFBCcAAAAABYzqAAAAQDUzqlMRHScAAAAABQQnAAAAAAWM6gAAAEA1KxvVqYSOEwAAAIACghMAAACAAkZ1AAAAoJrZVaciOk4AAAAACghOAAAAAAoY1QEAAIBqZlediug4AQAAACggOAEAAAAoYFQHAAAAqplddSqi4wQAAACggOAEAAAAoIBRHQAAAKhmdtWpiI4TAAAAgAKCEwAAAIACRnUAAACgmtlVpyI6TgAAAAAKCE4AAAAAChjVAQAAgGpmVKciOk4AAAAACghOAAAAAAoY1QEAAIBqVi43dwUtmo4TAAAAgAKCEwAAAIACRnUAAACgmtlVpyI6TgAAAAAKCE4AAAAAChjVAQAAgGpmVKciOk4AAAAACghOAAAAAAoY1QEAAIBqVjaqUwkdJwAAAAAFBCcAAAAABQQnAAAAUM1qaxfOx3wYPXp0+vbtm/bt22fQoEG5//77v/T43/zmNxk4cGA6duyYnj17Zo899si0adMadU3BCQAAALDQu+GGG3LooYfmuOOOy8SJE7PRRhtl6623zqRJk+Z6/AMPPJBhw4Zlr732yrPPPpsbb7wxjz32WPbee+9GXVdwAgAAACz0zj333Oy1117Ze++9s+qqq+a8885Lr169cvHFF8/1+Icffjh9+vTJwQcfnL59++a//uu/su++++bxxx9v1HUFJwAAAFDNyuWF8jFjxox8+OGH9R4zZsyY61uYOXNmJkyYkK222qre+lZbbZUHH3xwrudssMEGefPNNzN27NiUy+W8++67+d3vfpdtt922UZ8+wQkAAADwtRs1alQ6depU7zFq1Ki5Hjt16tTMmTMn3bt3r7fevXv3vPPOO3M9Z4MNNshvfvObDB06NO3atUuPHj2yxBJL5IILLmhUnYITAAAA4Gs3cuTIfPDBB/UeI0eO/NJzSqVSvY/L5XKDtc8999xzOfjgg3PiiSdmwoQJufPOO/Paa69lv/32a1SdbRp1NAAAANCyzOcONgtaTU1Nampq5unYrl27pnXr1g26S6ZMmdKgC+Vzo0aNyoYbbpgjjzwySTJgwIAsssgi2WijjXL66aenZ8+e83RtHScAAADAQq1du3YZNGhQxo0bV2993Lhx2WCDDeZ6zr/+9a+0alU/9mjdunWSzzpV5pXgBAAAAFjojRgxIpdddlnGjBmT559/PocddlgmTZpUN3ozcuTIDBs2rO747bbbLjfffHMuvvjivPrqq/nrX/+agw8+OOuuu26WXnrpeb6uUR0AAACoZgvpqE5jDR06NNOmTcupp56ayZMnZ/XVV8/YsWPTu3fvJMnkyZMzadKkuuOHDx+ejz76KBdeeGEOP/zwLLHEEtlss81y1llnNeq6pXJj+lMWoFlTXmruEgCgReq47JDmLgEAWqRZM99q7hK+Fp9efkRzlzBXHfY6p7lLmCdGdQAAAAAKGNUBAACAalaujlGd5qLjBAAAAKCA4AQAAACggFEdAAAAqGLl2oViT5gWS8cJAAAAQAHBCQAAAEABozoAAABQzWrtqlOJijpO3njjjbz55ptNVQsAAADAQqXRwcns2bNzwgknpFOnTunTp0969+6dTp065fjjj8+sWbMWRI0AAAAAzaLRozoHHnhgbrnllpx99tkZPHhwkuShhx7KySefnKlTp+aSSy5p8iIBAACA+VQ2qlOJRgcn1113Xa6//vpsvfXWdWsDBgzIcsstl5133llwAgAAAFSNRo/qtG/fPn369Gmw3qdPn7Rr164pagIAAABYKDQ6ODnggANy2mmnZcaMGXVrM2bMyBlnnJEDDzywSYsDAAAAKlRbXjgfLUSjR3UmTpyYe+65J8suu2wGDhyYJHnyySczc+bMbL755tlxxx3rjr355pubrlIAAACAr1mjg5Mlllgi3/ve9+qt9erVq8kKAgAAAFhYNDo4ueKKKxZEHQAAAMCCUGtXnUo0+h4nAAAAAN8Uje446du3b0qlUuHzr776akUFAQAAACwsGh2cHHroofU+njVrViZOnJg777wzRx55ZFPVBQAAADQFozoVaXRwcsghh8x1/aKLLsrjjz9ecUEAAAAAC4smu8fJ1ltvnZtuuqmpXg4AAACg2TW646TI7373u3Tu3LmpXg4AAABoCuVyc1fQojU6OFlrrbXq3Ry2XC7nnXfeyXvvvZfRo0c3aXEAAAAAzanRwckOO+xQ7+NWrVplqaWWypAhQ7LKKqs0VV0AAAAAza7RwclJJ520IOoAAAAAFgS76lSk0TeHHTt2bO66664G63fddVfuuOOOJikKAAAAYGHQ6ODkmGOOyZw5cxqsl8vlHHPMMU1SFAAAAMDCoNGjOi+99FL69+/fYH2VVVbJyy+/3CRFAQAAAE2k1q46lWh0x0mnTp3y6quvNlh/+eWXs8giizRJUQAAAAALg0YHJ9tvv30OPfTQvPLKK3VrL7/8cg4//PBsv/32TVocAAAAQHNqdHDy05/+NIssskhWWWWV9O3bN3379s2qq66aLl265JxzzlkQNQJN7Ppb/phv77RX1t78u9lpr0My4clnvvT4626+Pdv9cL8M2nzHfGfXffP7O+9pcMyvf/v7fGfXfTNo8x2z+feG56xf/CozZsxcUG8BABa4/fbdPS++8FA++vCVPPLwHdlww3W/9PiNNlo/jzx8Rz768JW88PcH86N9dqv3fP/+/XLDDZfmpRcfzqyZb+Xgg/Zu8BqfP/fFxy/OP6NJ3xvwDVOuXTgfLUSj73HSqVOnPPjggxk3blyefPLJdOjQIQMGDMjGG2+8IOoDmtgd9/wlZ/7iVzl+xP5Za43+ufG2O7LfkSfntl+PTs/u3Rocf/0tY3PeL6/KyUcdlNVX7Zenn3shJ599YTottmiGbLhekuT2u8fn57+8Mqcdc0jWXH3VvP7GWzn+J+clSY4+eJ+v8+0BQJP4wQ+2z89+dnIOOujYPPjQY9ln791y+x+uyYCBQ/LGG283OL5Pn175w22/zuWXX5vdhx+UDQZ/Kxdc8JO8N3VabrllbJKkY4cOee3VSbnppttzzk9Pnut1B2+wTVq3bl338WqrrZK77rw+v7vp9gXyPgH4ao0OTpKkVCplq622ylZbbdXU9QAL2NU33Jodt90y39/u20mSYw7+Uf766BO5/paxOWy/4Q2O/8Pdf84Ptt86W2/+WTjaa+keeeq5F3L5b26qC06efPbvWWv1VbPtlkOSJMv07J5tttg4Tz//4tfyngCgqR16yD654orrM+aK65Ikhx9xUrbcapPsu++wHH/8mQ2O/9GPdsukN97K4UeclCT5+99fzqBBAzPisP3qgpPHJzyZxyc8mSQ54/Rj53rdqVOn1/v4qCMPzMsvv5a//OWhJntvADTOfAUn99xzT+65555MmTIltbX122vGjBnTJIUBTW/WrFl57sWXs9cPv19vfYNvrZUnn/n73M+ZOSs1NW3rrdW0a5enn38xs2bPTts2bbLWGv1z+9335unnXsga/VfOG2+/k788/Hj+5783X2DvBQAWlLZt22bttQfk7J9eVG/9T+Puy+D115nrOeuvNyh/GndfvbW7x92bPfbYOW3atMns2bPnq45dd90x551/aaPPBajHrjoVaXRwcsopp+TUU0/NOuusk549e6ZUKi2IuoAF4J8ffJg5c2rTZckl6613WXLJTJ3+xFzP2WDdtXPTH+7OZhsNTv9+K+TZF17OLWP/lNmzZ+f99z/MUl07Z5stNsk/3/8wux1wdFIuZ/acORm6wzbZ+4c/+DreFgA0qa5dO6dNmzaZ8u7UeuvvTpma7j0ajrUmSfce3fLulPrHT3l3atq2bZuuXTvnnXemNLqO//mf/84SSyyeq6/+baPPBaDpNDo4ueSSS3LllVdmt912++qDC8yYMSMzZsyot9ZqxszU1LSb79cE5t0X885yyoUh6H7Dd87U6f/M/+57eMopp8uSS2SHrTfPmGtvSqvWn91f+tGJT+XSX9+Q40fsnwH9V86kt97Omef/Kkt1uS77Dd9lQb8dAFggyuX6f6EtlUoN1r78+Lmvz6s9hu+cO+8an8mT352v8wFoGo3eVWfmzJnZYIMNKrroqFGj0qlTp3qPs35xSUWvCXy1JTstntatW2Xq9H/WW5/+z/fTZckl5npO+5qanD7y0Dz2p5ty12/HZNzvrsjSPbpnkY4dsmSnxZMkF152TbbbarN8f7tvp98KfbLFxhvkkB8Ny2XX/K7BOB8ALOymTp2e2bNnp3uPpeqtd1uqS6a8+95cz3n3nSnp0b3+8Ut165pZs2Zl2rR/zvWcL7Pccstk8803ypgx1zb6XIAvKtfWLpSPlqLRwcnee++da6+t7P/AR44cmQ8++KDe4+iD96voNYGv1rZt2/Tvt2Ieeuxv9dYfeuxvGbj6Kl9+bps26dGta1q3bp077/lLNtlg3bRq9dn/hfz73zPSqlX9jpXWrVulXC7P91/ZAKC5zJo1K0888VS22Lz+rpGbb7FxHnr48bme8/AjE7L5FvWP33KLTTJhwlPzdX+T3XcfmilTpmbs2HsafS4ATWueRnVGjBhR9++1tbW59NJL86c//SkDBgxI27b1bxp57rnnfuXr1dTUpKampt7arH8b04Gvw7ChO2Tk6edmtVVWzMDVVs3vbrszk6e8l6E7bJMk+fklV2bK1GkZdfzhSZLXJ72Vp59/MQP698uHH32cq264NS+99o+ccdxhda+5yYbr5uobbs0qKy3//0d1JueCy67JkP9ar96WigDQUpx3/q9y5RXnZ8KEJ/PwIxOy914/zHK9lsmll/46SXL66cdkmaV7Zo89D0mSXHrpr/Pj/ffIT88+KZeP+U3WX29Q9thj5/xwtwPqXrNt27bp379fkqRdu7ZZeukeGThwtXz88Sd55ZXX644rlUrZfdjQ/PqaGzNnzpyv700DMFfzFJxMnDix3sdrrrlmkuSZZ55p8oKABWvrzTfOBx9+lEuuvD7vTZuelfr2zsVnn5yl///N7qZO+2cm/0cb8pza2lx1wy15fdJbadOmddZda0CuufinWaZn97pj9h22c0qlUi647JpMeW9allyiU4ZsuG4O3mf+74UEAM3pxhtvS5fOS+a44w5Lz57d8uyzL2S77XfLpElvJUl69uieXr2Wrjv+9dffyHbb75afnXNy9t9/97z99rs57LAT67YiTpKll+6exx+7u+7jww/fP4cfvn/uu+/BbLHl/91QffPNN0rv3svmyitv+BreKfCNYFedipTKC0kf/awpLzV3CQDQInVcdkhzlwAALdKsmW81dwlfi0/OGNbcJczVIsdd3dwlzJNG3+Nkzz33zEcffdRg/ZNPPsmee+7ZJEUBAAAALAwaHZxcddVV+fTTTxusf/rpp7n66paRFgEAAMA3Rrl24Xy0EPN0j5Mk+fDDD+t2yPjoo4/Svn37uufmzJmTsWPHplu3bgukSAAAAIDmMM/ByRJLLJFSqZRSqZR+/fo1eL5UKuWUU05p0uIAAAAAmtM8Byfjx49PuVzOZpttlptuuimdO3eue65du3bp3bt3ll566S95BQAAAOBrZ1edisxzcLLJJpskSV577bX06tUrrVo1+vYoAAAAAC3KPAcnn+vdu3fef//9XH755Xn++edTKpXSv3//7LnnnunUqdOCqBEAAACgWTS6beTxxx/PCiuskJ///OeZPn16pk6dmnPPPTcrrLBCnnjiiQVRIwAAADC/amsXzkcL0eiOk8MOOyzbb799fvWrX6VNm89Onz17dvbee+8ceuih+ctf/tLkRQIAAAA0h0YHJ48//ni90CRJ2rRpk6OOOirrrLNOkxYHAAAA0JwaPaqz+OKLZ9KkSQ3W33jjjSy22GJNUhQAAADQRGrLC+ejhWh0cDJ06NDstddeueGGG/LGG2/kzTffzPXXX5+99947u+yyy4KoEQAAAKBZNHpU55xzzkmpVMqwYcMye/bsJEnbtm2z//7758wzz2zyAgEAAACaS6lcLs9Xf8y//vWvvPLKKymXy1lxxRXTsWPHigqZNeWlis4HgG+qjssOae4SAKBFmjXzreYu4WvxyQk7NXcJc7XIab9t7hLmSaM7Tj7XsWPHLLnkkimVShWHJgAAAAALo0bf46S2tjannnpqOnXqlN69e2e55ZbLEksskdNOOy21LWgfZgAAAICv0uiOk+OOOy6XX355zjzzzGy44YYpl8v561//mpNPPjn//ve/c8YZZyyIOgEAAID50YJ2sFkYNTo4ueqqq3LZZZdl++23r1sbOHBglllmmfz4xz8WnAAAAABVo9GjOtOnT88qq6zSYH2VVVbJ9OnTm6QoAAAAgIVBo4OTgQMH5sILL2ywfuGFF2bgwIFNUhQAAADQNMq1tQvlo6Vo9KjO2WefnW233TZ/+tOfMnjw4JRKpTz44IN54403Mnbs2AVRIwAAAECzaHTHSd++ffPiiy/mu9/9bt5///1Mnz49O+64Y1544YX07t17QdQIAAAA0Cwa3XHSt2/fTJ48ucFNYKdNm5ZevXplzpw5TVYcAAAAUCG76lSk0R0n5fLcP+Eff/xx2rdvX3FBAAAAAAuLee44GTFiRJKkVCrlxBNPTMeOHeuemzNnTh555JGsueaaTV4gAAAAQHOZ5+Bk4sSJST7rOHn66afTrl27uufatWuXgQMH5ogjjmj6CgEAAID5Z1SnIvMcnIwfPz5Jsscee+T888/P4osvvsCKAgAAAFgYNPrmsFdcccWCqAMAAABgodPo4AQAAABoQcq1zV1Bi9boXXUAAAAAvikEJwAAAAAFjOoAAABANbOrTkV0nAAAAAAUEJwAAAAAFDCqAwAAAFWsbFSnIjpOAAAAAAoITgAAAAAKGNUBAACAamZUpyI6TgAAAAAKCE4AAAAAChjVAQAAgGpWW9vcFbRoOk4AAAAACghOAAAAAAoY1QEAAIBqZlediug4AQAAACggOAEAAAAoYFQHAAAAqplRnYroOAEAAAAoIDgBAAAAKGBUBwAAAKpYuWxUpxI6TgAAAAAKCE4AAAAAChjVAQAAgGpmV52K6DgBAAAAKCA4AQAAAChgVAcAAACqmVGdiug4AQAAACggOAEAAAAoYFQHAAAAqljZqE5FdJwAAAAAFBCcAAAAABQwqgMAAADVzKhORXScAAAAABQQnAAAAAAUMKoDAAAA1ay2uQto2XScAAAAABQQnAAAAAAUMKoDAAAAVaxsV52K6DgBAAAAKCA4AQAAAChgVAcAAACqmVGdiug4AQAAACggOAEAAAAoYFQHAAAAqlltcxfQsuk4AQAAACggOAEAAAAoYFQHAAAAqljZrjoV0XECAAAAUEBwAgAAAFDAqA4AAABUM7vqVETHCQAAAEABwQkAAABAAaM6AAAAUMXsqlMZHScAAAAABQQnAAAAAAWM6gAAAEA1s6tORXScAAAAABQQnAAAAAAUMKoDAAAAVaxsVKciOk4AAAAACghOAAAAAAoY1QEAAIBqZlSnIjpOAAAAAAoITgAAAAAKGNUBAACAKmZXncroOAEAAAAoIDgBAAAAKGBUBwAAAKqZUZ2K6DgBAAAAKCA4AQAAAChgVAcAAACqmF11KqPjBAAAAKCA4AQAAACggFEdAAAAqGJGdSqj4wQAAABoEUaPHp2+ffumffv2GTRoUO6///4vPX7GjBk57rjj0rt379TU1GSFFVbImDFjGnVNHScAAADAQu+GG27IoYcemtGjR2fDDTfML3/5y2y99dZ57rnnstxyy831nJ122invvvtuLr/88qy44oqZMmVKZs+e3ajrlsrlcrkp3kClZk15qblLAIAWqeOyQ5q7BABokWbNfKu5S/havLvpJs1dwlx1H39fo45fb731svbaa+fiiy+uW1t11VWzww47ZNSoUQ2Ov/POO7Pzzjvn1VdfTefOnee7TqM6AAAAwEJt5syZmTBhQrbaaqt661tttVUefPDBuZ5z2223ZZ111snZZ5+dZZZZJv369csRRxyRTz/9tFHXNqoDAAAAfO1mzJiRGTNm1FurqalJTU1Ng2OnTp2aOXPmpHv37vXWu3fvnnfeeWeur//qq6/mgQceSPv27XPLLbdk6tSp+fGPf5zp06c36j4nOk4AAACgmpVLC+Vj1KhR6dSpU73H3EZu/lOpVKr/1srlBmufq62tTalUym9+85usu+662WabbXLuuefmyiuvbFTXiY4TAAAA4Gs3cuTIjBgxot7a3LpNkqRr165p3bp1g+6SKVOmNOhC+VzPnj2zzDLLpFOnTnVrq666asrlct58882stNJK81SnjhMAAADga1dTU5PFF1+83qMoOGnXrl0GDRqUcePG1VsfN25cNthgg7mes+GGG+btt9/Oxx9/XLf24osvplWrVll22WXnuU7BCQAAAFSxcu3C+WisESNG5LLLLsuYMWPy/PPP57DDDsukSZOy3377Jfmsg2XYsGF1x++6667p0qVL9thjjzz33HP5y1/+kiOPPDJ77rlnOnToMM/XNaoDAAAALPSGDh2aadOm5dRTT83kyZOz+uqrZ+zYsendu3eSZPLkyZk0aVLd8YsuumjGjRuXgw46KOuss066dOmSnXbaKaeffnqjrlsql8vlJn0n82nWlJeauwQAaJE6LjukuUsAgBZp1sy3mruEr8U7Gw9p7hLmqsdf7m3uEuaJjhMAAACoYuXaue86w7xxjxMAAACAAoITAAAAgAJGdQAAAKCKzc8ONvwfHScAAAAABQQnAAAAAAWM6gAAAEAVK5ftqlMJHScAAAAABQQnAAAAAAWM6gAAAEAVs6tOZXScAAAAABQQnAAAAAAUMKoDAAAAVaxca1edSug4AQAAACggOAEAAAAoYFQHAAAAqli53NwVtGw6TgAAAAAKCE4AAAAAChjVAQAAgCpmV53K6DgBAAAAKCA4AQAAAChgVAcAAACqmFGdyug4AQAAACggOAEAAAAoYFQHAAAAqli53NwVtGw6TgAAAAAKCE4AAAAAChjVAQAAgCpmV53K6DgBAAAAKCA4AQAAAChgVAcAAACqWLlsVKcSOk4AAAAACghOAAAAAAoY1QEAAIAqVq5t7gpaNh0nAAAAAAUEJwAAAAAFjOoAAABAFau1q05FdJwAAAAAFBCcAAAAABQwqgMAAABVrGxUpyI6TgAAAAAKCE4AAAAAChjVAQAAgCpWrjWqUwkdJwAAAAAFBCcAAAAABYzqAAAAQBUrl5u7gpZNxwkAAABAAcEJAAAAQAGjOgAAAFDF7KpTGR0nAAAAAAUEJwAAAAAFjOoAAABAFastG9WphI4TAAAAgAKCEwAAAIACRnUAAACgipWN6lRExwkAAABAAcEJAAAAQAGjOgAAAFDFyuXmrqBl03ECAAAAUEBwAgAAAFDAqA4AAABUsVq76lRExwkAAABAAcEJAAAAQAGjOgAAAFDFykZ1KqLjBAAAAKCA4AQAAACggFEdAAAAqGLlcnNX0LLpOAEAAAAoIDgBAAAAKGBUBwAAAKpYrV11KqLjBAAAAKCA4AQAAACgwEIzqtNh2SHNXQIAtEifvn1/c5cAACzEykZ1KqLjBAAAAKCA4AQAAACgwEIzqgMAAAA0PbvqVEbHCQAAAEABwQkAAABAAaM6AAAAUMXKzV1AC6fjBAAAAKCA4AQAAACggFEdAAAAqGJ21amMjhMAAACAAoITAAAAgAJGdQAAAKCKlY3qVETHCQAAAEABwQkAAABAAaM6AAAAUMVqm7uAFk7HCQAAAEABwQkAAABAAaM6AAAAUMXKsatOJXScAAAAABQQnAAAAAAUMKoDAAAAVay23NwVtGw6TgAAAAAKCE4AAAAAChjVAQAAgCpWa1ediug4AQAAACggOAEAAAAoYFQHAAAAqljZqE5FdJwAAAAAFBCcAAAAABQwqgMAAABVrLa5C2jhdJwAAAAAFBCcAAAAABQwqgMAAABVzK46ldFxAgAAAFBAcAIAAABQwKgOAAAAVDG76lRGxwkAAABAAcEJAAAAQAGjOgAAAFDFjOpURscJAAAAQAHBCQAAAEABozoAAABQxcopNXcJLZqOEwAAAIACghMAAACAAkZ1AAAAoIrVmtSpiI4TAAAAgAKCEwAAAIACRnUAAACgitXaVaciOk4AAAAACghOAAAAAAoY1QEAAIAqVm7uAlo4HScAAAAABQQnAAAAAAWM6gAAAEAVq23uAlo4HScAAAAABQQnAAAAAAWM6gAAAEAVqy2VmruEFk3HCQAAAEABwQkAAABAAaM6AAAAUMXKzV1AC6fjBAAAAKCA4AQAAACggFEdAAAAqGK1zV1AC6fjBAAAAKCA4AQAAACggFEdAAAAqGK1peauoGXTcQIAAABQQHACAAAAUMCoDgAAAFSx2pjVqYSOEwAAAIACghMAAACAAkZ1AAAAoIqVm7uAFk7HCQAAAEABwQkAAABAAcEJAAAAVLHa0sL5mB+jR49O37590759+wwaNCj333//PJ3317/+NW3atMmaa67Z6GsKTgAAAICF3g033JBDDz00xx13XCZOnJiNNtooW2+9dSZNmvSl533wwQcZNmxYNt988/m6ruAEAAAAWOide+652WuvvbL33ntn1VVXzXnnnZdevXrl4osv/tLz9t133+y6664ZPHjwfF1XcAIAAABVrHYhfcyYMSMffvhhvceMGTPm+h5mzpyZCRMmZKuttqq3vtVWW+XBBx8sfO9XXHFFXnnllZx00knz+NlqSHACAAAAfO1GjRqVTp061XuMGjVqrsdOnTo1c+bMSffu3eutd+/ePe+8885cz3nppZdyzDHH5De/+U3atGkz33XO/5kAAAAA82nkyJEZMWJEvbWampovPadUqn9X2XK53GAtSebMmZNdd901p5xySvr161dRnYITAAAAqGLl5i6gQE1NzVcGJZ/r2rVrWrdu3aC7ZMqUKQ26UJLko48+yuOPP56JEyfmwAMPTJLU1tamXC6nTZs2ufvuu7PZZpvN07WN6gAAAAALtXbt2mXQoEEZN25cvfVx48Zlgw02aHD84osvnqeffjp/+9vf6h777bdfVl555fztb3/LeuutN8/X1nECAAAALPRGjBiR3XbbLeuss04GDx6cSy+9NJMmTcp+++2X5LPRn7feeitXX311WrVqldVXX73e+d26dUv79u0brH8VwQkAAABUsdqGtwBpkYYOHZpp06bl1FNPzeTJk7P66qtn7Nix6d27d5Jk8uTJmTRpUpNft1QulxeKcac27ZZp7hIAoEX69O37m7sEAGiR2nZdvrlL+FpcvuwPm7uEudrrzWuau4R54h4nAAAAAAUEJwAAAAAF3OMEAAAAqlhtcxfQwuk4AQAAACggOAEAAAAoYFQHAAAAqphRncroOAEAAAAoIDgBAAAAKGBUBwAAAKpYudTcFbRsOk4AAAAACghOAAAAAAoY1QEAAIAqZledyug4AQAAACggOAEAAAAoYFQHAAAAqphRncroOAEAAAAoIDgBAAAAKGBUBwAAAKpYubkLaOF0nAAAAAAUEJwAAAAAFDCqAwAAAFWsttTcFbRsOk4AAAAACghOAAAAAAoY1QEAAIAqVtvcBbRwOk4AAAAACghOAAAAAAoY1QEAAIAqZlSnMjpOAAAAAAoITgAAAAAKGNUBAACAKlZu7gJaOB0nAAAAAAUEJwAAAAAFjOoAAABAFastNXcFLZuOEwAAAIACghMAAACAAkZ1AAAAoIrVNncBLZyOEwAAAIACghMAAACAAkZ1AAAAoIqVm7uAFk7HCQAAAEABwQkAAABAAaM6AAAAUMVqDetURMcJAAAAQAHBCQAAAEABozoAAABQxWqbu4AWTscJAAAAQAHBCQAAAEABozoAAABQxeypUxkdJwAAAAAFBCcAAAAABYzqAAAAQBWzq05ldJwAAAAAFBCcAAAAABQwqgMAAABVrLbU3BW0bDpOAAAAAAoITgAAAAAKGNUBAACAKlabcnOX0KLpOAEAAAAoIDgBAAAAKGBUBwAAAKqYQZ3K6DgBAAAAKCA4AQAAAChgVAcAAACqWG1zF9DC6TgBAAAAKCA4AQAAAChgVAcAAACqWK19dSqi4wQAAACggOAEAAAAoIBRHQAAAKhiBnUqo+MEAAAAoIDgBAAAAKCAUR0AAACoYrXNXUALp+MEAAAAoIDgBAAAAKCAUR0AAACoYrX21amIjhMAAACAAoITAAAAgAJGdQAAAKCKGdSpjI4TAAAAgAKCEwAAAIACRnUAAACgitU2dwEtXKM6TmbNmpVNN900L7744oKqBwAAAGCh0ajgpG3btnnmmWdSKpUWVD0AAAAAC41G3+Nk2LBhufzyyxdELQAAAEATKy+k/7QUjb7HycyZM3PZZZdl3LhxWWeddbLIIovUe/7cc89tsuIAAAAAmlOjg5Nnnnkma6+9dpI0uNeJER4AAACgmjQ6OBk/fvyCqAMAAABYAOyqU5lG3+Pkcy+//HLuuuuufPrpp0mScrnlzCcBAAAAzItGByfTpk3L5ptvnn79+mWbbbbJ5MmTkyR77713Dj/88CYvEAAAAKC5NDo4Oeyww9K2bdtMmjQpHTt2rFsfOnRo7rzzziYtDgAAAKhMbcoL5aOlaPQ9Tu6+++7cddddWXbZZeutr7TSSvnHP/7RZIUBAAAANLdGd5x88skn9TpNPjd16tTU1NQ0SVEAAAAAC4NGBycbb7xxrr766rqPS6VSamtr89Of/jSbbrppkxYHAAAAVKa8kD5aikaP6vz0pz/NkCFD8vjjj2fmzJk56qij8uyzz2b69On561//uiBqBAAAAGgWje446d+/f5566qmsu+662XLLLfPJJ59kxx13zMSJE7PCCissiBoBAAAAmkWjO06SpEePHjnllFOauhYAAACgibWkHWwWRvMUnDz11FPz/IIDBgyY72IAAAAAFibzFJysueaaKZVKKZe/PKUqlUqZM2dOkxQGAAAA0NzmKTh57bXXFnQdAAAAwAJQ29wFtHDzFJz07t17QdcBAAAAsNCZr5vDvvDCC7ngggvy/PPPp1QqZZVVVslBBx2UlVdeuanrAwAAAGg2jd6O+He/+11WX331TJgwIQMHDsyAAQPyxBNPZPXVV8+NN964IGoEKrDfvrvnpRceyscfvpJHHr4j/7Xhul96/MYbrZ9HHr4jH3/4Sl78+4P50T671Xu+f/9++e0Nl+blFx/O7Jlv5eCD9m7wGhv913q59ZYrM+n1CZk9861sv/23m/Q9AUBzuf7m2/Pt7w/P2ptun532PCgT/vbMlx5/3U1/yHa7/iiDNv2ffGfnvfP7O/5U7/lZs2fn4jG/yX//YI+sven22XH3H+eBhx9fkG8B+AYqL6T/tBSNDk6OOuqojBw5Mg899FDOPffcnHvuuXnwwQdz7LHH5uijj14QNQLz6Qc/2D7n/uzkjDrzF1ln3W/ngQceze1/uCa9ei091+P79OmVP9z26zzwwKNZZ91v58yzLsh5Pz813/3uNnXHdOzQIa+9OinHHv+TTJ787lxfZ5FFOuapp57LwYcev0DeFwA0hzv+dF/OPP+X2WfYzrnxiguz9oDVst8RJ2TyO1Pmevz1t9ye8y65Ij/e839z6zWX5Md7/zBn/Gx07n3g4bpjLrj0qtz4+zty7GH75/fX/DI77bBNDhl5Wp5/8eWv620B8BVK5a/aKucLOnbsmKeeeiorrrhivfWXXnopAwcOzL/+9a/5KqRNu2Xm6zyg2IMP/CFPTHwmBx40sm7t6afuzW233Znjjj+zwfGjfnJsvvOdrbLGgCF1axddeGYGDuif/9p4+wbHv/ziw/nFBZflFxdcVljD7JlvZcfv75nbbrursjcDFPr07fubuwT4Rthln0Ozar8VcuKRB9Wtbbfrj7LZRoNz2P57NDj+f/cdkbXW6J8jDvy/7swzz7skz77wUn598c+SJJtu/7/50e47Z5fvbVd3zMHHnJoOHdrnrJOOWoDvBkiStl2Xb+4SvhZ79/l+c5cwV5e9/rvmLmGeNLrjZMiQIbn//oa/oD3wwAPZaKONmqQooHJt27bN2msPyLg/3Vdvfdy4+zJ4/XXmes766w3KuHH1j7973L0ZNGhA2rSZr1siAUBVmDVrVp574aVssO7a9dY3WHftPPnMc4Xn1LRrV2+tpqYmTz/3YmbNnp0kmTlrVto1OKZdJj71bBNWD3zT1S6kj5Zinv5L6Lbbbqv79+233z5HH310JkyYkPXXXz9J8vDDD+fGG2/MKaecsmCqBBqta9fOadOmTaa8O7Xe+pQpU9O9R7e5ntO9R7dMmfKF49+dmrZt26Zr1855p6AVGQCq3T/f/zBz5tSmS+cl6613WXKJTJ32z7mes8G6g3LT7Xdms40Hp//KK+bZv7+UW/54d2bPnp333/8wS3XtnA3XG5Srr78566y5enot0zMPP/63jL//4cypnfN1vC0A5sE8BSc77LBDg7XRo0dn9OjR9dYOOOCA7Lfffl/5ejNmzMiMGTPqrZXL5ZRKpXkpB2iEL07jlUqlBmtffvzc1wHgm+iLv6+WU/w77H577JKp06fnf390WMopp8uSS2aHbbbImN/8Lq1af9b4fcwh++bks36R7Xb9UUqlpNfSPbPDtlvm1j+OW+DvBYB5M0/BSW1t0zbRjBo1qkF3SqnVoim1XrxJrwPfZFOnTs/s2bPTvcdS9daXWqpLprz73lzPefedKene/QvHd+uaWbNmZVrBX9MA4JtgySUWT+vWrTJ12vR669P/+UG6dF5irue0r6nJ6ceOyElHHZxp0/+Zpbp0zo233ZFFOnbIkp0++72385JL5BdnnpgZM2bm/Q8/TLeuXfLzi8dkmZ7dF/RbAr5BWtIONgujRt/jpCmMHDkyH3zwQb1HqdVizVEKVK1Zs2bliSeeyhabb1xvfYstNs5DBdscPvzIhGyxRf3jt9xik0yY8FRm//9ZbAD4Jmrbtm36r7xSHnpsYr31hx57IgNX7//l57Zpkx7dlkrr1q1z55/uyyYbrpdWrer/Gl5T0y7dl+qa2XPmZNy9f82mGw1u8vcAwPyZr7s9Pvroo7n33nszZcqUBt0o55577leeX1NTk5qamnprxnSg6f38/F/lqivOz4QJT+bhRyZkn71+mOV6LZNfXvrrJMkZpx+TpZfumT32PCRJ8stLf50f779Hzjn7pFw25jdZf71B2XOPnfO/ux1Q95pt27ZN//79kiTt2rXNMkv3yMCBq+Xjjz/JK6+8nuSz7YhXXLFv3Tl9+yyXgQNXy/Tp/8wbb7z9Nb17AGhaw4Z+NyNPOyerrbJSBq6+an73+zsy+d33MvS72yRJfn7xFZkydVpGnXBEkuT1SW/m6edfzID+K+fDjz7OVdffnJde/UfOOP6Iutd86tm/5933pmWVlZbPlPemZfSYa1Iul7Pn/y6cO2AAfBM1Ojj5yU9+kuOPPz4rr7xyunfvXi/wEH7AwuXGG29Ll85L5vjjDkvPnt3yzLMvZLvtd8ukSW8lSXr06J7lei1dd/zrr7+R7bbfLeecc3L233/3vP32uzn0sBNzyy1j645ZeunumfDY3XUfH374/jn88P1z330PZvMtf5AkWWfQwNzzp//bWuxn55ycJLnq6t9mr70PW5BvGQAWmK232CQffPhRLrni2rw3bXpWWr5PLj7n1Czd47OxmqnTpmfyu/93I/U5tbW56rqb8vqkt9KmTeusu/bAXHPJufXGcGbMnJkLfnVV3nz7nXTs0CEbDf5WRp1wZBZfbNGv/f0B1asl7WCzMCqVG3nHx+7du+ess87K8OHDm7SQNu2WadLXA4Bvik/fvr+5SwCAFqlt1+Wbu4Svxe59vtfcJczVVa/f1NwlzJNG3+OkVatW2XDDDRdELQAAAAALlUYHJ4cddlguuuiiBVELAAAA0MRqy+WF8tFSNPoeJ0cccUS23XbbrLDCCunfv3/atm1b7/mbb765yYoDAAAAaE6NDk4OOuigjB8/Pptuumm6dOnihrAAAABA1Wp0cHL11Vfnpptuyrbbbrsg6gEAAACaUMsZilk4NfoeJ507d84KK6ywIGoBAAAAWKg0Ojg5+eSTc9JJJ+Vf//rXgqgHAAAAYKHR6FGdX/ziF3nllVfSvXv39OnTp8HNYZ944okmKw4AAACoTK1hnYo0OjjZYYcdFkAZAAAAAAufRgcnJ5100oKoAwAAAGCh0+jgBAAAAGg5ykZ1KtLo4KRVq1YplUqFz8+ZM6eiggAAAAAWFo0OTm655ZZ6H8+aNSsTJ07MVVddlVNOOaXJCgMAAABobo0OTv7nf/6nwdr3v//9rLbaarnhhhuy1157NUlhAAAAQOVqm7uAFq5VU73Qeuutlz/96U9N9XIAAAAAza5JgpNPP/00F1xwQZZddtmmeDkAAACAhUKjR3WWXHLJejeHLZfL+eijj9KxY8dcc801TVocAAAAUJlau+pUpNHByXnnnZc5c+akdevWST7bZWeppZbKeuutl48++qjJCwQAAABoLo0OTvbcc89Mnjw53bp1q7c+bdq09O3b13bEAAAAQNVodHBSLpfrjep87uOPP0779u2bpCgAAACgaZSN6lRknoOTESNGJElKpVJOOOGEdOzYse65OXPm5JFHHsmaa67Z5AUCAAAANJd5Dk4mTpyY5LOOk6effjrt2rWre65du3YZOHBgjjjiiKavEAAAAKCZzHNwMn78+CTJHnvskfPPPz+LL774AisKAAAAaBq1zV1AC9foe5xcccUVC6IOAAAAgIVOq+YuAAAAAGBh1eiOEwAAAKDlKJftqlMJHScAAAAABQQnAAAAAAWM6gAAAEAVq41RnUroOAEAAAAoIDgBAAAAKGBUBwAAAKpYbXMX0MLpOAEAAAAoIDgBAAAAKCA4AQAAgCpWXkj/mR+jR49O37590759+wwaNCj3339/4bE333xzttxyyyy11FJZfPHFM3jw4Nx1112NvqbgBAAAAFjo3XDDDTn00ENz3HHHZeLEidloo42y9dZbZ9KkSXM9/i9/+Uu23HLLjB07NhMmTMimm26a7bbbLhMnTmzUdUvlcnmh2NC5TbtlmrsEAGiRPn27+C8tAECxtl2Xb+4SvhbfWW7b5i5hrm6f9MdGHb/eeutl7bXXzsUXX1y3tuqqq2aHHXbIqFGj5uk1VltttQwdOjQnnnjiPF/XrjoAAABQxWrncyxmQZsxY0ZmzJhRb62mpiY1NTUNjp05c2YmTJiQY445pt76VlttlQcffHCerldbW5uPPvoonTt3blSdRnUAAACAr92oUaPSqVOneo+izpGpU6dmzpw56d69e7317t2755133pmn6/3sZz/LJ598kp122qlRdeo4AQAAAL52I0eOzIgRI+qtza3b5D+VSqV6H5fL5QZrc3Pdddfl5JNPzu9///t069atUXUKTgAAAKCKLSS3Nm2gaCxnbrp27ZrWrVs36C6ZMmVKgy6UL7rhhhuy11575cYbb8wWW2zR6DqN6gAAAAALtXbt2mXQoEEZN25cvfVx48Zlgw02KDzvuuuuy/Dhw3Pttddm223n7ya5Ok4AAACAhd6IESOy2267ZZ111sngwYNz6aWXZtKkSdlvv/2SfDb689Zbb+Xqq69O8lloMmzYsJx//vlZf/3167pVOnTokE6dOs3zdQUnAAAAUMVqm7uAJjJ06NBMmzYtp556aiZPnpzVV189Y8eOTe/evZMkkydPzqRJk+qO/+Uvf5nZs2fngAMOyAEHHFC3vvvuu+fKK6+c5+uWygvJsFObdss0dwkA0CJ9+vb9zV0CALRIbbsu39wlfC2+3Wvr5i5hru56447mLmGeuMcJAAAAQAGjOgAAAFDFylkoBk1aLB0nAAAAAAUEJwAAAAAFjOoAAABAFas1qlMRHScAAAAABQQnAAAAAAWM6gAAAEAVK5eN6lRCxwkAAABAAcEJAAAAQAGjOgAAAFDF7KpTGR0nAAAAAAUEJwAAAAAFjOoAAABAFSsb1amIjhMAAACAAoITAAAAgAJGdQAAAKCK1ZaN6lRCxwkAAABAAcEJAAAAQAGjOgAAAFDFDOpURscJAAAAQAHBCQAAAEABozoAAABQxWoN61RExwkAAABAAcEJAAAAQAGjOgAAAFDFjOpURscJAAAAQAHBCQAAAEABozoAAABQxcplozqV0HECAAAAUEBwAgAAAFDAqA4AAABUMbvqVEbHCQAAAEABwQkAAABAAaM6AAAAUMXKRnUqouMEAAAAoIDgBAAAAKCAUR0AAACoYuWyUZ1K6DgBAAAAKCA4AQAAAChgVAcAAACqWK1ddSqi4wQAAACggOAEAAAAoIBRHQAAAKhidtWpjI4TAAAAgAKCEwAAAIACRnUAAACgitlVpzI6TgAAAAAKCE4AAAAAChjVAQAAgCpWNqpTER0nAAAAAAUEJwAAAAAFjOoAAABAFastG9WphI4TAAAAgAKCEwAAAIACRnUAAACgitlVpzI6TgAAAAAKCE4AAAAAChjVAQAAgCpmV53K6DgBAAAAKCA4AQAAAChgVAcAAACqmF11KqPjBAAAAKCA4AQAAACggFEdAAAAqGJ21amMjhMAAACAAoITAAAAgAJGdQAAAKCK2VWnMjpOAAAAAAoITgAAAAAKGNUBAACAKmZXncroOAEAAAAoIDgBAAAAKGBUBwAAAKqYXXUqo+MEAAAAoIDgBAAAAKCAUR0AAACoYuVybXOX0KLpOAEAAAAoIDgBAAAAKGBUBwAAAKpYrV11KqLjBAAAAKCA4AQAAACggFEdAAAAqGLlslGdSug4AQAAACggOAEAAAAoYFQHAAAAqphddSqj4wQAAACggOAEAAAAoIBRHQAAAKhidtWpjI4TAAAAgAKCEwAAAIACRnUAAACgitUa1amIjhMAAACAAoITAAAAgAJGdQAAAKCKlWNUpxI6TgAAAAAKCE4AAAAAChjVAQAAgCpWtqtORXScAAAAABQQnAAAAAAUMKoDAAAAVazWrjoV0XECAAAAUEBwAgAAAFDAqA4AAABUMbvqVEbHCQAAAEABwQkAAABAAaM6AAAAUMVqjepURMcJAAAAQAHBCQAAAEABozoAAABQxeyqUxkdJwAAAAAFBCcAAAAABYzqAAAAQBWrjVGdSug4AQAAACggOAEAAAAoYFQHAAAAqphddSqj4wQAAACggOAEAAAAoIBRHQAAAKhitUZ1KqLjBAAAAKCA4AQAAACggFEdAAAAqGLlGNWphI4TAAAAgAKCEwAAAIACRnUAAACgitlVpzI6TgAAAAAKCE4AAAAAChjVAQAAgCpWNqpTER0nAAAAAAUEJwAAAAAFjOoAAABAFSvHqE4ldJwAAAAAFBCcAAAAABQwqgMAAABVzK46ldFxAgAAAFBAcAIAAABQwKgOAAAAVDGjOpXRcQIAAABQQHACAAAAUMCoDgAAAFQxgzqV0XECAAAAUEBwAgAAAFCgVHZ7XeBLzJgxI6NGjcrIkSNTU1PT3OUAQIvhZyhAdRCcAF/qww8/TKdOnfLBBx9k8cUXb+5yAKDF8DMUoDoY1QEAAAAoIDgBAAAAKCA4AQAAACggOAG+VE1NTU466SQ3tQOARvIzFKA6uDksAAAAQAEdJwAAAAAFBCcAAAAABQQnAAAAAAUEJ0ChIUOG5NBDD23uMgCgRSuVSrn11lsLn3/99ddTKpXyt7/97WurCYB5JziBFmZBhBn33ntvSqVS3n///SZ9XQBYWPnjAADzSnACAAAt3MyZM5u7BICqJTiBFmT48OG57777cv7556dUKqVUKuX111/Pfffdl3XXXTc1NTXp2bNnjjnmmMyePbvuvBkzZuTggw9Ot27d0r59+/zXf/1XHnvssSSftQdvuummSZIll1wypVIpw4cPrzu3trY2Rx11VDp37pwePXrk5JNPrlfT+++/nx/96Efp3r172rdvn9VXXz233357kuTkk0/OmmuuWe/48847L3369Kn3nnbYYYeccsop6datWxZffPHsu+++fgEEYIGZ28/TK6+8MksssUS942699daUSqW6jz//uTZmzJgst9xyWXTRRbP//vtnzpw5Ofvss9OjR49069YtZ5xxRoNrTp48OVtvvXU6dOiQvn375sYbb/zSGp977rlss802WXTRRdO9e/fstttumTp1at3zQ4YMyYEHHpgRI0aka9eu2XLLLSv7pABQSHACLcj555+fwYMHZ5999snkyZMzefLktG3bNttss02+9a1v5cknn8zFF1+cyy+/PKeffnrdeUcddVRuuummXHXVVXniiSey4oor5tvf/namT5+eXr165aabbkqSvPDCC5k8eXLOP//8unOvuuqqLLLIInnkkUdy9tln59RTT824ceOSfBaqbL311nnwwQdzzTXX5LnnnsuZZ56Z1q1bN+p93XPPPXn++eczfvz4XHfddbnllltyyimnNMFnDAAamtvP0zlz5szTua+88kruuOOO3HnnnbnuuusyZsyYbLvttnnzzTdz33335ayzzsrxxx+fhx9+uN55J5xwQr73ve/lySefzA9/+MPssssuef755+d6jcmTJ2eTTTbJmmuumccffzx33nln3n333ey00071jrvqqqvSpk2b/PWvf80vf/nL+ftkAPCV2jR3AcC869SpU9q1a5eOHTumR48eSZLjjjsuvXr1yoUXXphSqZRVVlklb7/9do4++uiceOKJ+fTTT3PxxRfnyiuvzNZbb50k+dWvfpVx48bl8ssvz5FHHpnOnTsnSbp169bgr20DBgzISSedlCRZaaWVcuGFF+aee+7JlltumT/96U959NFH8/zzz6dfv35JkuWXX77R76tdu3YZM2ZMOnbsmNVWWy2nnnpqjjzyyJx22mlp1Uq+C0DTmtvP03kN/WtrazNmzJgstthi6d+/fzbddNO88MILGTt2bFq1apWVV145Z511Vu69996sv/76def94Ac/yN57750kOe200zJu3LhccMEFGT16dINrXHzxxVl77bXzk5/8pG5tzJgx6dWrV1588cW6n7krrrhizj777Pn+PAAwbwQn0MI9//zzGTx4cL1W4g033DAff/xx3nzzzbz//vuZNWtWNtxww7rn27Ztm3XXXbfwL13/acCAAfU+7tmzZ6ZMmZIk+dvf/pZll1227he4+TVw4MB07Nix7uPBgwfn448/zhtvvJHevXtX9NoA0JT69OmTxRZbrO7j7t27p3Xr1vWC/u7du9f9rPzc4MGDG3xctIvOhAkTMn78+Cy66KINnnvllVfqfu6us8468/s2AGgEwQm0cOVyuV5o8vla8tn2h//571913ty0bdu23selUim1tbVJkg4dOnzpua1ataq7/udmzZr1ldf8z2sBwNdhXn9mze3n4pf9rPwyRT/namtrs9122+Wss85q8FzPnj3r/n2RRRb5ymsAUDk98NDCtGvXrt4cdv/+/fPggw/W+2XvwQcfzGKLLZZlllkmK664Ytq1a5cHHnig7vlZs2bl8ccfz6qrrlr3mknmeb77cwMGDMibb76ZF198ca7PL7XUUnnnnXfq1Ta3v649+eST+fTTT+s+fvjhh7Poootm2WWXbVQ9ADCvvvjzdKmllspHH32UTz75pG6tqCNkfnzxnicPP/xwVllllbkeu/baa+fZZ59Nnz59suKKK9Z7CEsAvn6CE2hh+vTpk0ceeSSvv/56pk6dmh//+Md54403ctBBB+Xvf/97fv/73+ekk07KiBEj0qpVqyyyyCLZf//9c+SRR+bOO+/Mc889l3322Sf/+te/stdeeyVJevfunVKplNtvvz3vvfdePv7443mqZZNNNsnGG2+c733vexk3blxee+21uhvmJZ/d8f+9997L2WefnVdeeSUXXXRR7rjjjgavM3PmzOy111557rnncscdd+Skk07KgQce6P4mACwwX/x5ut5666Vjx4459thj8/LLL+faa6/NlVde2WTXu/HGGzNmzJi8+OKLOemkk/Loo4/mwAMPnOuxBxxwQKZPn55ddtkljz76aF599dXcfffd2XPPPRv9Rw4AKue/SqCFOeKII9K6dev0798/Sy21VGbNmpWxY8fm0UcfzcCBA7Pffvtlr732yvHHH193zplnnpnvfe972W233bL22mvn5Zdfzl133ZUll1wySbLMMsvklFNOyTHHHJPu3bsX/iI3NzfddFO+9a1vZZdddkn//v1z1FFH1f1St+qqq2b06NG56KKLMnDgwDz66KM54ogjGrzG5ptvnpVWWikbb7xxdtppp2y33XYNtj0GgKb0xZ+nH374Ya655pqMHTs2a6yxRq677rom/Vl0yimn5Prrr8+AAQNy1VVX5Te/+U369+8/12OXXnrp/PWvf82cOXPy7W9/O6uvvnoOOeSQdOrUyR8VAJpBqfzFYU6Ar9Hw4cPz/vvv59Zbb23uUgAAABoQWQMAAAAUEJwAAAAAFDCqAwAAAFBAxwkAAABAAcEJAAAAQAHBCQAAAEABwQkAAABAAcEJAAAAQAHBCQAAAEABwQkAAABAAcEJAAAAQAHBCQAAAECB/wdISpY79tSXQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "test_acc = np.sum(test.label == test.pred) / len(test)\n",
    "test_matrix = confusion_matrix(test['label'], test['pred'])\n",
    "epoch_f1 = f1_score(test['label'], test['pred'], average='macro')\n",
    "print(f'accuracy: {test_acc:.4f}')\n",
    "print(f'f1_score: {epoch_f1:.4f}')\n",
    "\n",
    "test_matrix = confusion_matrix(test['label'], test['pred'], normalize='true')\n",
    "#test_matrix = confusion_matrix(test['label'], test['pred'])\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(test_matrix, \n",
    "            annot=True, \n",
    "            xticklabels = sorted(set(test['label'])), \n",
    "            yticklabels = sorted(set(test['label'])),\n",
    "            )\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#print(f'confusion_matrix \\n-------------------------\\n {test_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162577b-8bd7-4cfb-b7e4-7332c1959181",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
